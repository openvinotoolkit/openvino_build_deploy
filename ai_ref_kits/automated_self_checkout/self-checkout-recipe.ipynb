{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e550b39c-f84f-4ac8-94b7-5623724557bc",
   "metadata": {},
   "source": [
    "# Detect and Track Objects with OpenVINO™ for Self-Checkout\n",
    "\n",
    "Automated self-checkout is a popular application centered around improving shoppers’ experiences through expedited check-out experiences. Consumers can easily grasp an object, place it in a shopping cart, or scan the object in a self-checkout kiosk and purchase the item with minimal contact, allowing for increased operational efficiency.unting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95f099a-23bb-4c03-bcf6-7bfd3340b138",
   "metadata": {},
   "source": [
    "In this article, you’ll learn how to use OpenVINO™ with Ultralytics’ YOLOv8 and Roboflow’s supervision libraries to create the fundamentals of an Automated Self-Checkout system. This application offers a short and easily modifiable implementation to detect and tracks objects in a zone and provide real-time analytics data regarding whether the object was added or removed from the zone and the IDs of the person who interact with the objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20093565-419d-4428-8d39-329c4950171a",
   "metadata": {},
   "source": [
    "Because the zone definition is flexible, retailers can define custom zones depending on the type of self-checkout they would like to perform, such as:\n",
    "\n",
    "-\tSelf-checkout counters with designated areas for placing, removing, and bagging the item.\n",
    "-\tDefining a zone on shelves to identify how many objects are removed from shelves for theft detecion.\n",
    "-\tWithin shopping carts to identify how many items users add/remove from the platform/shelf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74a2785-3443-4967-868e-47755f2cb95f",
   "metadata": {},
   "source": [
    "This application leverages the YOLOv8 model and optimization process and tackles similar elements for zone definition also described in **Intelligent Queue Management Edge AI Reference Kit**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbe295-0777-4a6d-b2dd-9afca8a2794f",
   "metadata": {},
   "source": [
    "> NOTE: This notebook involves performing object detection and tracking on a video clip, for accurate definition of the polygone zone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d510730-ceab-4aa0-946a-6cd055709331",
   "metadata": {},
   "source": [
    "# Imports and Dependencies Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "dc6c71ce-b870-4558-8812-be294758d384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "\n",
    "from IPython import display\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93d7fce1-dd43-41df-b320-bb8201d06415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import logging as log\n",
    "import json\n",
    "\n",
    "log.basicConfig(level=log.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fa3058-d00a-49e8-bfb8-76f18e7ebab1",
   "metadata": {},
   "source": [
    "# Loading our OpenVINO™ YOLO model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3420c560-0653-48be-9e14-90929470d4d7",
   "metadata": {},
   "source": [
    "YOLOv8 provides API for convenient model exporting to different formats, including OpenVINO IR. `model.export` is responsible for model conversion. We need to specify the format, and additionally, we could preserve dynamic shapes in the model. It would limit us to use CPU only, so we're not doing this. Also, we specify we want to use half-precision (FP16) to get better performance.\n",
    "\n",
    "Let's load our OpenVINO YOLOv8 model FP16 model via the Ultralytics API for faster inference with a smaller model footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5176a81-64ee-456e-8efb-93e7711811a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.117  Python-3.10.6 torch-1.13.1+cpu CPU\n",
      "YOLOv8m summary (fused): 218 layers, 25886080 parameters, 0 gradients, 78.9 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from model\\yolov8m.pt with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (49.7 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.0 opset 16...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  3.2s, saved as model\\yolov8m.onnx (99.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m starting export with openvino 2023.0.0-10926-b4452d56304-releases/2023/0...\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m export success  3.7s, saved as model\\yolov8m_openvino_model\\ (49.8 MB)\n",
      "\n",
      "Export complete (9.1s)\n",
      "Results saved to \u001b[1mC:\\Users\\rcheruvu\\Desktop\\openvino_notebooks\\recipes\\automated_detection_tracking\\model\u001b[0m\n",
      "Predict:         yolo predict task=detect model=model\\yolov8m_openvino_model imgsz=640 \n",
      "Validate:        yolo val task=detect model=model\\yolov8m_openvino_model imgsz=640 data=coco.yaml \n",
      "Visualize:       https://netron.app\n",
      "WARNING  Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify', or 'pose'.\n"
     ]
    }
   ],
   "source": [
    "#Specify our models path\n",
    "models_dir = Path('./model')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "DET_MODEL_NAME = \"yolov8m\"\n",
    "det_model = YOLO(models_dir / f'{DET_MODEL_NAME}.pt')\n",
    "label_map = det_model.model.names\n",
    "\n",
    "# Load our Yolov8 object detection model\n",
    "ov_model_path = Path(f\"model/{DET_MODEL_NAME}_openvino_model/{DET_MODEL_NAME}.xml\")\n",
    "if not ov_model_path.exists():\n",
    "    # export model to OpenVINO format\n",
    "    out_dir = det_model.export(format=\"openvino\", dynamic=False, half=True)\n",
    "\n",
    "model = YOLO('model/yolov8m_openvino_model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734c1f0f-b117-4f60-9aa3-8b0b982b01c3",
   "metadata": {},
   "source": [
    "# Define and Load a Zone\n",
    "\n",
    "In order to accurately define a zone for our input video clip, we can extract a single video frame from our clip using the Supervision library, and drag and drop it into [Roboflow's open-source Polygon Zone tool](https://roboflow.github.io/polygonzone/) to define the coordinates of our zone.\n",
    "\n",
    "Let's start with loading in a sample video and extracting a single frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98ad8ac8-1a62-4c4b-9408-db86f7c7ce51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoInfo(width=3840, height=2160, fps=29, total_frames=640)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in our sample video\n",
    "VID_PATH = \"data/example.mp4\"\n",
    "#Show the dimensions and additional information from the video\n",
    "video_info = sv.VideoInfo.from_video_path(VID_PATH)\n",
    "video_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cb899ca-737b-4626-a494-0f7101898482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract a single frame from the video\n",
    "generator = sv.get_video_frames_generator(VID_PATH)\n",
    "iterator = iter(generator)\n",
    "frame = next(iterator)\n",
    "#Save the frame\n",
    "cv2.imwrite(\"frame.jpg\", frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc046bd-69b4-4a05-b9dc-3f35cfe889c9",
   "metadata": {},
   "source": [
    "Next, we can navigate over to the Polygon Zone tool to extract the coordinates and incorporate them into the zones.json file.\n",
    "\n",
    "We've already included two example configurations as part of the zones.json file that you can also readily leverage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707f6d09-bd59-4e71-befc-34a73a4f145d",
   "metadata": {},
   "source": [
    "![Roboflow Tool snapshot](https://github.com/openvinotoolkit/openvino_notebooks/assets/22090501/51d8ef0f-ff7a-42c8-b755-5aaaae9a3a11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d33b32-c539-4ce6-835f-7463140b3080",
   "metadata": {},
   "source": [
    "Next, let's load our zone coordinates. The following function takes a path to a JSON file that defines zones and their boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f470947-5887-422b-81b4-5cf2b4ed64cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zones(json_path, zone_str):\n",
    "    \"\"\"\n",
    "        Load zones specified in an external json file\n",
    "        Parameters:\n",
    "            json_path: path to the json file with defined zones\n",
    "            zone_str:  name of the zone in the json file\n",
    "        Returns:\n",
    "           zones: a list of arrays with zone points\n",
    "    \"\"\"\n",
    "    # load json file\n",
    "    with open(json_path) as f:\n",
    "        zones_dict = json.load(f)\n",
    "    # return a list of zones defined by points\n",
    "    return np.array(zones_dict[zone_str][\"points\"], np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccca375c-b219-4e50-93a8-75fe6e8811c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 776,  321],\n",
       "       [3092,  305],\n",
       "       [3112, 1965],\n",
       "       [ 596, 2005],\n",
       "       [ 768,  321]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygon = load_zones(\"zones.json\", \"test-example-1\")\n",
    "polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87111c2b-1abd-4544-9438-a42891c2e5d9",
   "metadata": {},
   "source": [
    "We can now create PolygonZone, PolygonZoneAnnotator, and BoxAnnotator objects for each zone based on the polygon coordinates we determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bff6ee2-d595-42a6-8491-4377f8cf3b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone = sv.PolygonZone(polygon=polygon, frame_resolution_wh=video_info.resolution_wh)\n",
    "box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n",
    "zone_annotator = sv.PolygonZoneAnnotator(zone=zone, color=sv.Color.white(), thickness=6, text_thickness=6, text_scale=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a0e66-cac8-4cba-a4ec-e74a29502854",
   "metadata": {},
   "source": [
    "# Define Helper Functions\n",
    "\n",
    "In this section, we'll define a few helper functions that can helps us with the flow of our self-checkout pipeline.\n",
    "\n",
    "The `draw_text()` function calculates the size of the text and the size of the rectangle that will be drawn around the text based on the image size. It uses the `cv2.rectangle()` function to draw the rectangle and the `cv2.putText()` function to draw the text. We'll need this function to be able to overlay text on our video stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb92d49c-45d4-4f04-83af-411a1d92f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_text(image, text, point, color=(255, 255, 255)) -> None:\n",
    "    \"\"\"\n",
    "    Draws text\n",
    "\n",
    "    Parameters:\n",
    "        image: image to draw on\n",
    "        text: text to draw\n",
    "        point:\n",
    "        color: text color\n",
    "    \"\"\"\n",
    "    _, f_width = image.shape[:2]\n",
    "    \n",
    "    text_size, _ = cv2.getTextSize(text, fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=2, thickness=2)\n",
    "\n",
    "    rect_width = text_size[0] + 20\n",
    "    rect_height = text_size[1] + 20\n",
    "    rect_x, rect_y = point\n",
    "\n",
    "    cv2.rectangle(image, pt1=(rect_x, rect_y), pt2=(rect_x + rect_width, rect_y + rect_height), color=(255, 255, 255), thickness=cv2.FILLED)\n",
    "\n",
    "    text_x = (rect_x + (rect_width - text_size[0]) // 2) - 10\n",
    "    text_y = (rect_y + (rect_height + text_size[1]) // 2) - 10\n",
    "    \n",
    "    cv2.putText(image, text=text, org=(text_x, text_y), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=2, color=color, thickness=2, lineType=cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d74548-2661-407d-a594-70d2a6e29df8",
   "metadata": {},
   "source": [
    "The `get_iou()` function calculates the Intersection Over Union score using the `xyxy` coordinates of two bounding boxes corresponding to two detected objects. In this case, we will use the `get_iou()` function to identify if the detected bounding boxes for a person intersects with a detected object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca866c33-7363-4232-a97f-1eccfe72b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(person_det, object_det):\n",
    "    #Obtain the Intersection \n",
    "    x_left = max(person_det[0], object_det[0])\n",
    "    y_top = max(person_det[1], object_det[1])\n",
    "    x_right = min(person_det[2], object_det[2])\n",
    "    y_bottom = min(person_det[3], object_det[3])\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    person_area = (person_det[2] - person_det[0]) * (person_det[3] - person_det[1])\n",
    "    obj_area = (object_det[2] - object_det[0]) * (object_det[3] - object_det[1])\n",
    "    \n",
    "    return intersection_area / float(person_area + obj_area - intersection_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c16abe-8b1f-4d94-94d6-1108a89c79aa",
   "metadata": {},
   "source": [
    "The `intersecting_bboxes()` function identifies if the bounding boxes for people and objects are intersecting leveraging the above function, and logs the appropriate interaction accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7706a43-c90e-450e-a6eb-38360fd6ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersecting_bboxes(bboxes, person_bbox, action_str):\n",
    "    #Identify if person and object bounding boxes are intersecting using IOU\n",
    "    for box in bboxes:\n",
    "      if box.cls == 0:\n",
    "          #If it is a person\n",
    "          try:\n",
    "              person_bbox.append([box.xyxy[0], box.id.numpy().astype(int)])\n",
    "          except:\n",
    "              pass\n",
    "      elif box.cls != 0 and len(person_bbox) >= 1:\n",
    "          #If it is not a person and an interaction took place with a person\n",
    "          for p_bbox in person_bbox:\n",
    "              if box.cls != 0:\n",
    "                  result_iou = get_iou(p_bbox[0], box.xyxy[0])\n",
    "                  if result_iou > 0:\n",
    "                     try:\n",
    "                        person_intersection_str = f\"Person #{p_bbox[1][0]} interacted with object #{int(box.id[0])} {label_map[int(box.cls[0])]}\"\n",
    "                     except:\n",
    "                         person_intersection_str = f\"Person {p_bbox[1][0]} interacted with object (ID unable to be assigned) {label_map[int(box.cls[0])]}\"\n",
    "                     #log.info(person_intersection_str)\n",
    "                     person_action_str = action_str + f\" by person {p_bbox[1][0]}\"\n",
    "                     return person_action_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbfa74c-62ff-4981-b0fb-00a4d802694d",
   "metadata": {},
   "source": [
    "# Run the Main Processing Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09ece60-a7a2-4202-afab-00b4c34807ff",
   "metadata": {},
   "source": [
    "Run object detection and tracking on the specified video clip.\n",
    "\n",
    "To customize the tracking algorithm, visit [https://docs.ultralytics.com/modes/track/#tracker-selection](https://docs.ultralytics.com/modes/track/#tracker-selection) to learn more about the default algorithm and additional option. \n",
    "\n",
    "Note that there are a few misses that can occur with object detection and tracking algorithms in this use case: \n",
    "\n",
    "- The off-the-shelf object detection algorithm sometimes does not immediately detect objects that are present, and can take a few frames to do so\n",
    "- The off-the-shelf tracking algorithm can sometimes assign multiple IDs for the same object (and even in some cases multiple objects the same ID). It's important to keep these potential mistakes the algorithm can make in mind, and consider a custom tracking algorithm (using the details in the link above) to be able to customize the algorithm for your use case if these elements impact your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0e5f85-b2ee-4d23-a9e9-e522bf83d97e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define empty lists to keep track of labels\n",
    "original_labels = []\n",
    "final_labels = []\n",
    "person_bbox = []\n",
    "p_items = []\n",
    "purchased_items = set(p_items)\n",
    "a_items = []\n",
    "added_items = set(a_items)\n",
    "\n",
    "#Save result as det_tracking_result\n",
    "with sv.VideoSink(\"new_det_tracking_result.mp4\", video_info) as sink:\n",
    "    #Iterate through model predictions and tracking results\n",
    "    for index, result in enumerate(model.track(source=VID_PATH, show=False, stream=True, verbose=True, persist=True)):\n",
    "      #Define variables to store interactions that are refreshed per frame\n",
    "      interactions = []\n",
    "      person_intersection_str = \"\"\n",
    "\n",
    "      #Obtain predictions from yolov8 model\n",
    "      frame = result.orig_img\n",
    "      detections = sv.Detections.from_ultralytics(result)\n",
    "      detections = detections[detections.class_id < 55]\n",
    "      mask = zone.trigger(detections=detections)\n",
    "      detections_filtered = detections[mask]\n",
    "      bboxes = result.boxes\n",
    "      if bboxes.id is not None:\n",
    "          detections.tracker_id = bboxes.id.cpu().numpy().astype(int)\n",
    "        \n",
    "      labels = [\n",
    "          f'#{tracker_id} {label_map[class_id]} {confidence:0.2f}'\n",
    "          for _, _, confidence, class_id, tracker_id\n",
    "          in detections\n",
    "      ]\n",
    "\n",
    "      #Annotate the frame with the zone and bounding boxes.\n",
    "      frame = box_annotator.annotate(scene=frame, detections=detections_filtered, labels=labels)\n",
    "      frame = zone_annotator.annotate(scene=frame)\n",
    "\n",
    "      objects = [f'#{tracker_id} {label_map[class_id]}' for _, _, confidence, class_id, tracker_id in detections]\n",
    "\n",
    "      #If this is the first time we run the application,\n",
    "      #store the objects' labels as they are at the beginning\n",
    "      if index == 0:\n",
    "          original_labels = objects\n",
    "          original_dets = len(detections_filtered)\n",
    "      else:\n",
    "          #To identify if an object has been added or removed\n",
    "          #we'll use the original labels and identify any changes\n",
    "          final_labels = objects\n",
    "          new_dets = len(detections_filtered)\n",
    "          #Identify if an object has been added or removed using Counters\n",
    "          removed_objects = Counter(original_labels) - Counter(final_labels)\n",
    "          added_objects = Counter(final_labels) - Counter(original_labels)\n",
    "\n",
    "          #Create two variables we can increment for drawing text\n",
    "          draw_txt_ir = 1\n",
    "          draw_txt_ia = 1\n",
    "          #Check for objects being added or removed\n",
    "          if new_dets - original_dets != 0 and len(removed_objects) >= 1:\n",
    "             #An object has been removed\n",
    "              for k,v in removed_objects.items():\n",
    "                 #For each of the objects, check the IOU between a designated object\n",
    "                 #and a person.\n",
    "                 if 'person' not in k:\n",
    "                     removed_object_str = f\"{v} {k} removed from zone\"\n",
    "                     removed_action_str = intersecting_bboxes(bboxes, person_bbox, removed_object_str)\n",
    "                     if removed_action_str is not None:\n",
    "                         #If we have determined an interaction with a person,\n",
    "                         #log the interaction.\n",
    "                         log.info(removed_action_str)\n",
    "                         #Add the purchased items to a \"receipt\" of sorts\n",
    "                         if removed_object_str not in purchased_items:\n",
    "                             #print(f\"{v} {k}\", a_items)\n",
    "                             #if f\"{v} {k}\" in a_items:\n",
    "                             purchased_items.add(f\"{v} {k}\")\n",
    "                             p_items.append(f\" - {v} {k}\")\n",
    "                     #Draw the result on the screen        \n",
    "                     draw_text(frame, text=removed_action_str, point=(50, 50 + draw_txt_ir), color=(0, 0, 255))\n",
    "                     draw_txt_ir += 80\n",
    "          \n",
    "          if len(added_objects) >= 1:\n",
    "              #An object has been added\n",
    "              for k,v in added_objects.items():\n",
    "                  #For each of the objects, check the IOU between a designated object\n",
    "                  #and a person.\n",
    "                  if 'person' not in k:\n",
    "                      added_object_str = f\"{v} {k} added to zone\"\n",
    "                      added_action_str = intersecting_bboxes(bboxes, person_bbox, added_object_str)\n",
    "                      if added_action_str is not None:\n",
    "                          #If we have determined an interaction with a person,\n",
    "                          #log the interaction.\n",
    "                          log.info(added_action_str)\n",
    "                          if added_object_str not in added_items:\n",
    "                            added_items.add(added_object_str)\n",
    "                            a_items.append(added_object_str)\n",
    "                      #Draw the result on the screen  \n",
    "                      draw_text(frame, text=added_action_str, point=(50, 300 + draw_txt_ia), color=(0, 128, 0))\n",
    "                      draw_txt_ia += 80\n",
    "      \n",
    "      draw_text(frame, \"Receipt: \" + str(purchased_items), point=(50, 800), color=(30, 144, 255))\n",
    "      sink.write_frame(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226a4e5c",
   "metadata": {},
   "source": [
    "Sample output:\n",
    "\n",
    "> video 1/1 (1/640) automated_detection_tracking\\data\\asset.mp4: 640x640 1 bottle, 1 banana, 1 apple, 949.0ms\n",
    "> \n",
    "> video 1/1 (2/640) automated_detection_tracking\\data\\asset.mp4: 640x640 1 bottle, 1 banana, 1 apple, 930.7ms\n",
    "> \n",
    "> video 1/1 (3/640) automated_detection_tracking\\data\\asset.mp4: 640x640 1 bottle, 1 banana, 1 apple, 1100.5ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36af19bd-bd35-4587-b118-4814f2140d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Receipt: set()'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Receipt: \" + str(purchased_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5bb93c6a-7a71-4ed1-97ef-45baa2717e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'#29 apple': 1, '#27 bottle': 1, '#25 banana': 1})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "added_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbd5a72-bc16-4b8a-9ea5-d6d5b943c584",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fcb66b-eba9-40ee-baf5-aac5bc65e8dc",
   "metadata": {},
   "source": [
    "For more information on how to performance benchmark OpenVINO YOLOv8 models, visit this [notebook](https://github.com/openvinotoolkit/openvino_notebooks/blob/recipes/recipes/intelligent_queue_management/docs/convert-and-optimize-the-model.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ac121-9b7d-44cf-b401-9d06f8ce8a47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
