{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c70e1eeb-961a-4a86-9916-c03be198cc30",
   "metadata": {},
   "source": [
    "# Chatbots with OpenVINO GenAI\n",
    "\n",
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b898e8-0ad0-4a56-9641-5f73af9e418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openvino-genai huggingface-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c00c95-fc40-4c0c-b657-6cbf7cd1c66c",
   "metadata": {},
   "source": [
    "## Download preconverted and preoptimized Qwen3-8B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64863dac-12ab-40d7-814e-1e5de28ec9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/repos/openvino_build_deploy/trainings/large_language_models/venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:186: UserWarning: The `resume_download` argument is deprecated and ignored in `snapshot_download`. Downloads always resume whenever possible.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b210a8651704005b746cf40b1f04da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636128ee27b744c08cbe5f2a9e3b3a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/adrian/repos/openvino_build_deploy/trainings/large_language_models/models/qwen3-8B'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "llm_output_dir = \"models/qwen3-8B\"\n",
    "snapshot_download(\"OpenVINO/Qwen3-8B-int4-cw-ov\", local_dir=llm_output_dir, resume_download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1431d58-b9d5-47b5-8677-574d4584d284",
   "metadata": {},
   "source": [
    "## Query devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a814ead-fd0c-4db8-986e-30462dd599da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CPU', 'GPU', 'NPU']\n",
      "['Intel(R) Core(TM) Ultra 7 258V', 'Intel(R) Arc(TM) Graphics (iGPU)', 'Intel(R) AI Boost']\n"
     ]
    }
   ],
   "source": [
    "import openvino as ov\n",
    "\n",
    "core = ov.Core()\n",
    "available_devices = core.available_devices\n",
    "\n",
    "print(available_devices)\n",
    "print([core.get_property(device, \"FULL_DEVICE_NAME\") for device in available_devices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e988b9c1-d013-4303-aa21-5f8d769c2ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e249b7b4638b4ed4bdb2bf96a4a2b814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Inference device:', index=1, options=('CPU', 'GPU', 'NPU'), value='GPU')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "# Select the model type\n",
    "device_dropdown = widgets.Dropdown(\n",
    "    options=available_devices,\n",
    "    value=\"GPU\" if \"GPU\" in available_devices else \"CPU\",\n",
    "    description=\"Inference device:\"\n",
    ")\n",
    "device_dropdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c2476-d588-4d98-b14f-7307209b5661",
   "metadata": {},
   "source": [
    "## Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f4a72bd-53e3-4b30-ac92-546163175477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino_genai as ov_genai\n",
    "\n",
    "llm_pipe = ov_genai.LLMPipeline(llm_output_dir, device_dropdown.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9422024a-3478-4566-8408-7bade2c711e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "OpenVINO is a cross-platform development toolkit by Intel that optimizes and accelerates the execution of deep learning workloads, particularly for inference tasks, to run efficiently on Intel hardware. It allows developers to deploy AI models on a wide range of devices, from high-performance servers to low-power edge devices, by optimizing model inference through a combination of hardware acceleration, model optimization, and a rich set of tools for model development, tuning, and deployment.\n"
     ]
    }
   ],
   "source": [
    "print(llm_pipe.generate(\"/no_think What is OpenVINO? Answer in one paragraph\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce072bc-b852-4fb4-a72f-02295fdef088",
   "metadata": {},
   "source": [
    "### Stream the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d0d26e-9221-4dea-8570-bcfe640de40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ov_genai.GenerationConfig()\n",
    "config.max_new_tokens = 1000\n",
    "\n",
    "prompt = \"/no_think Write a poem about Intel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ba1772b-25b4-43f0-a33a-17c789cba495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "**\"Ode to Intel\"**\n",
      "\n",
      "In circuits deep, where silence reigns,  \n",
      "A world of ones and zeros remains.  \n",
      "From silicon's embrace, a spark was born,  \n",
      "A vision of the future, a spark of morn.  \n",
      "\n",
      "From labs of old, where dreams were sown,  \n",
      "A titan rose from the soil of stone.  \n",
      "With logic and light, it carved its way,  \n",
      "A bridge between the mind and the day.  \n",
      "\n",
      "From vacuum tubes to chips of grace,  \n",
      "It shaped the world with every trace.  \n",
      "A dance of bits, a language of might,  \n",
      "It spoke in code, and yet it was right.  \n",
      "\n",
      "From mainframe's hum to micro's might,  \n",
      "It scaled the heights, both far and nigh.  \n",
      "A partner in the quest to know,  \n",
      "The secrets of the mind and the flow.  \n",
      "\n",
      "From Pentium's rise to threads of thread,  \n",
      "It wove the web, and made the world shed.  \n",
      "A click of the mouse, a screen's soft glow,  \n",
      "It shaped the world, and made it so.  \n",
      "\n",
      "From cloud to edge, from data to code,  \n",
      "It built the towers where the future's road.  \n",
      "A silent giant, yet so full of grace,  \n",
      "It moves the world with every trace.  \n",
      "\n",
      "So here's to Intel, with circuits so wide,  \n",
      "A beacon of progress, a guiding light.  \n",
      "From silicon's heart to the stars above,  \n",
      "It lights the way, and will never be wrong."
     ]
    }
   ],
   "source": [
    "def streamer(subword):\n",
    "    print(subword, end='', flush=True)\n",
    "    # Return flag corresponds whether generation should be stopped.\n",
    "    return ov_genai.StreamingStatus.RUNNING\n",
    "\n",
    "results = llm_pipe.generate([prompt], config, streamer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e96fe-8e08-45ec-8c54-9478bf5e4693",
   "metadata": {},
   "source": [
    "## Measure the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58d15405-01ef-43e3-8fec-6ccded58b9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output token size: 315\n",
      "Load time: 12705.00 ms\n",
      "Generate time: 20111.44 ± 0.00 ms\n",
      "Tokenization time: 14.55 ± 0.00 ms\n",
      "Detokenization time: 0.11 ± 0.00 ms\n",
      "Time to first token (TTFT): 1826.12 ± 0.00 ms\n",
      "Time per output token (TPOT): 58.23 ± 2.47 ms\n",
      "Throughput: 17.17 ± 0.73 tokens/s\n"
     ]
    }
   ],
   "source": [
    "perf_metrics = results.perf_metrics\n",
    "\n",
    "print(f\"Output token size: {perf_metrics.get_num_generated_tokens()}\")\n",
    "print(f\"Load time: {perf_metrics.get_load_time():.2f} ms\")\n",
    "print(f\"Generate time: {perf_metrics.get_generate_duration().mean:.2f} ± {perf_metrics.get_generate_duration().std:.2f} ms\")\n",
    "print(f\"Tokenization time: {perf_metrics.get_tokenization_duration().mean:.2f} ± {perf_metrics.get_tokenization_duration().std:.2f} ms\")\n",
    "print(f\"Detokenization time: {perf_metrics.get_detokenization_duration().mean:.2f} ± {perf_metrics.get_detokenization_duration().std:.2f} ms\")\n",
    "print(f\"Time to first token (TTFT): {perf_metrics.get_ttft().mean:.2f} ± {perf_metrics.get_ttft().std:.2f} ms\")\n",
    "print(f\"Time per output token (TPOT): {perf_metrics.get_tpot().mean:.2f} ± {perf_metrics.get_tpot().std:.2f} ms\")\n",
    "print(f\"Throughput: {perf_metrics.get_throughput().mean:.2f} ± {perf_metrics.get_throughput().std:.2f} tokens/s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
