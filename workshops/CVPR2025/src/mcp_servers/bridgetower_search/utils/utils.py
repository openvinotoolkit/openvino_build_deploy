from typing import Iterator, TextIO
from typing import List, Optional, Any
import PIL
import textwrap
import cv2
from io import StringIO
from os import path as osp
import json
import webvtt
from pathlib import Path
import base64
from moviepy.video.io.VideoFileClip import VideoFileClip
from .notebook_utils import device_widget

from langchain_core.embeddings import Embeddings
from mcp_servers.bridgetower_search.vectorstores.multimodal_lancedb import MultimodalLanceDB
import openvino_genai as ov_genai
# import whisper  # Commented out due to Windows compatibility issues - using OpenVINO whisper instead

def format_timestamp(seconds: float):
    """
    format time in srt-file expected format
    """
    assert seconds >= 0, "non-negative timestamp expected"
    milliseconds = round(seconds * 1000.0)

    hours = milliseconds // 3_600_000
    milliseconds -= hours * 3_600_000

    minutes = milliseconds // 60_000
    milliseconds -= minutes * 60_000

    seconds = milliseconds // 1_000
    milliseconds -= seconds * 1_000

    return (f"{hours}:" if hours > 0 else "00:") + f"{minutes:02d}:{seconds:02d}.{milliseconds:03d}"

# def format_timestamp(seconds: float, always_include_hours: bool = False, fractionalSeperator: str = '.'):
#     assert seconds >= 0, "non-negative timestamp expected"
#     milliseconds = round(seconds * 1000.0)

#     hours = milliseconds // 3_600_000
#     milliseconds -= hours * 3_600_000

#     minutes = milliseconds // 60_000
#     milliseconds -= minutes * 60_000

#     seconds = milliseconds // 1_000
#     milliseconds -= seconds * 1_000

#     hours_marker = f"{hours:02d}:" if always_include_hours or hours > 0 else ""
#     return f"{hours_marker}{minutes:02d}:{seconds:02d}{fractionalSeperator}{milliseconds:03d}"

# a help function that helps to convert a specific time written as a string in format `webvtt` into a time in miliseconds
def str2time(strtime):
    # strip character " if exists
    strtime = strtime.strip('"')
    # get hour, minute, second from time string
    hrs, mins, seconds = [float(c) for c in strtime.split(':')]
    # get the corresponding time as total seconds 
    total_seconds = hrs * 60**2 + mins * 60 + seconds
    total_miliseconds = total_seconds * 1000
    return total_miliseconds


def _processText(text: str, maxLineWidth=None):
    if (maxLineWidth is None or maxLineWidth < 0):
        return text

    lines = textwrap.wrap(text, width=maxLineWidth, tabsize=4)
    return '\n'.join(lines)


# Resizes a image and maintains aspect ratio
def maintain_aspect_ratio_resize(image, width=None, height=None, inter=cv2.INTER_AREA):
    # Grab the image size and initialize dimensions
    dim = None
    (h, w) = image.shape[:2]

    # Return original image if no need to resize
    if width is None and height is None:
        return image

    # We are resizing height if width is none
    if width is None:
        # Calculate the ratio of the height and construct the dimensions
        r = height / float(h)
        dim = (int(w * r), height)
    # We are resizing width if height is none
    else:
        # Calculate the ratio of the width and construct the dimensions
        r = width / float(w)
        dim = (width, int(h * r))

    # Return the resized image
    return cv2.resize(image, dim, interpolation=inter)

def write_vtt_for_openvino_whisper_model(transcript: Iterator[dict], file: TextIO, maxLineWidth=None):
    print("WEBVTT\n", file=file)
    # print(transcript[0])
    for segment in transcript:
        text = _processText(segment.text, maxLineWidth).replace('-->', '->')
        print(
            f"{format_timestamp(segment.start_ts)} --> {format_timestamp(segment.end_ts)}\n"
            f"{text}\n",
            file=file,
            flush=True,
        )

# helper function to convert transcripts generated by whisper to .vtt file
def write_vtt(transcript: Iterator[dict], file: TextIO, maxLineWidth=None):
    print("WEBVTT\n", file=file)
    # print(transcript[0])
    for segment in transcript:
        text = _processText(segment['text'], maxLineWidth).replace('-->', '->')
        print(
            f"{format_timestamp(segment['start'])} --> {format_timestamp(segment['end'])}\n"
            f"{text}\n",
            file=file,
            flush=True,
        )

# helper function to convert transcripts generated by whisper to .srt file
def write_srt(transcript: Iterator[dict], file: TextIO, maxLineWidth=None):
    """
    Write a transcript to a file in SRT format.
    Example usage:
        from pathlib import Path
        from whisper.utils import write_srt
        from moviepy.editor import VideoFileClip
        result = transcribe(model, audio_path, temperature=temperature, **args)
        # save SRT
        audio_basename = Path(audio_path).stem
        with open(Path(output_dir) / (audio_basename + ".srt"), "w", encoding="utf-8") as srt:
            write_srt(result["segments"], file=srt)
    """
    for i, segment in enumerate(transcript, start=1):
        text = _processText(segment['text'].strip(), maxLineWidth).replace('-->', '->')

        # write srt lines
        print(
            f"{i}\n"
            f"{format_timestamp(segment['start'], always_include_hours=True, fractionalSeperator=',')} --> "
            f"{format_timestamp(segment['end'], always_include_hours=True, fractionalSeperator=',')}\n"
            f"{text}\n",
            file=file,
            flush=True,
        )

def getSubs(segments: Iterator[dict], format: str, maxLineWidth: int=-1) -> str:
    segmentStream = StringIO()

    if format == 'vtt':
        write_vtt(segments, file=segmentStream, maxLineWidth=maxLineWidth)
    elif format == 'srt':
        write_srt(segments, file=segmentStream, maxLineWidth=maxLineWidth)
    else:
        raise Exception("Unknown format " + format)

    segmentStream.seek(0)
    return segmentStream.read()

# save temporary video file from bytes
def save_video_file(b64video: bytes, filename: str, path_to_save: str = ".") -> str:
    """
    Save video bytes to a temporary file and return the file path.
    
    Args:
        b64video (base64 bytes): The video file content encoded with base64.
    
    Returns:
        str: The path to the saved temporary video file.
    """
    
    # base64 decode the video bytes
    video = base64.b64decode(b64video)

    Path(osp.abspath(path_to_save)).mkdir(parents=True, exist_ok=True)
    full_path_fn = osp.join(path_to_save, filename)
    # if the file already exists, remove it
    if osp.exists(full_path_fn):
        print(f"File {full_path_fn} already exists, removing it.")
        Path(full_path_fn).unlink()
    # write the video bytes to the file
    with open(full_path_fn, "wb") as f:
        f.write(video)
    return full_path_fn

# extract audio from video bytes using cv2 and save it as a mp3 file
def video_to_audio(path_to_video: str, filename: str, path_to_save: str = ""):
    """
    Extract audio from video bytes and save it as a mp3 file.
    
    Args:
        video (bytes): The video file content.
        path_to_save_audio (str): The path to save the extracted audio file.
    """
    Path(osp.abspath(path_to_save)).mkdir(parents=True, exist_ok=True)

    if filename.endswith('.mp4'):
        filename = filename[:-4]  # remove .mp4 extension
    if not filename.endswith('.mp3'):
        filename = filename + ".mp3"
    output_audio_path = osp.join(path_to_save, filename)
    clip = VideoFileClip(path_to_video)
    audio = clip.audio
    audio.write_audiofile(output_audio_path)
    return output_audio_path

def extract_transcript_from_audio_with_openvino(
        path_to_audio: str, 
        filename: str,
        path_to_save: str, 
        model_dir: str):
    """
    Note: This will be deprecated in the future, use `extract_transcript_from_audio` instead. 
    This is due to the low performance of OpenVINO GenAI whisper model in identifying timestamps for transcripts.
    
    Extract transcript from audio file using OpenVINO GenAI whisper-small model
    Args:
        path_to_audio (str): Path to the audio file.
        path_to_save_transcript (str): Path to save the transcript file.
    """
    # Use environment variable for device configuration, fallback to CPU
    import os
    device = os.getenv("OPENVINO_WHISPER_MODEL_DEVICE", "CPU")
    print(f"[WHISPER MODEL] Initializing OpenVINO Whisper model...")
    print(f"[WHISPER MODEL] Model directory: {model_dir}")
    print(f"[WHISPER MODEL] Device: {device}")
    ov_pipe = ov_genai.WhisperPipeline(model_dir, device=device)
    print(f"[WHISPER MODEL] Whisper model loaded successfully")

    sampling_rate = 16000
    with open(path_to_audio, 'rb') as f:
        inputs = f.read()
    from transformers.pipelines.audio_utils import ffmpeg_read
    audio = ffmpeg_read(inputs, sampling_rate=sampling_rate)
    print(f"[WHISPER MODEL] Processing audio with task='transcribe'...")
    # Use "transcribe" instead of "translate" for better results with English audio
    transcription = ov_pipe.generate(audio, task="transcribe", return_timestamps=True).chunks
    print(f"[WHISPER MODEL] Transcription completed successfully")
    # save the transcript to a file
    if filename.endswith('.mp4'):
        filename = filename[:-4]  # remove .mp4 extension
    if not filename.endswith('.vtt'):
        filename = filename + ".vtt"
    output_transcript_path = osp.join(path_to_save, filename)
    write_vtt_for_openvino_whisper_model(transcription, file=open(output_transcript_path, 'w', encoding='utf-8'))
    return output_transcript_path

def extract_transcript_from_audio(
        path_to_audio: str, 
        filename: str,
        path_to_save: str, 
        model_dir: str):
    """
    Extract transcript from audio file using OpenVINO GenAI whisper-small model
    Args:
        path_to_audio (str): Path to the audio file.
        path_to_save_transcript (str): Path to save the transcript file.
    """
    import whisper  # Local import to avoid Windows compatibility issues at module level
    whisper_model = whisper.load_model(model_dir)
    # Use "transcribe" for English audio, "translate" for non-English to English
    options = dict(task="transcribe", best_of=1, language='en')
    results = whisper_model.transcribe(path_to_audio, **options)
    vtt = getSubs(results["segments"], "vtt")
    # # save the transcript to a file
    if filename.endswith('.mp4'):
        filename = filename[:-4]  # remove .mp4 extension
    if not filename.endswith('.vtt'):
        filename = filename + ".vtt"
    output_transcript_path = osp.join(path_to_save, filename)
    with open(output_transcript_path, 'w', encoding='utf-8') as f:
        f.write(vtt)
    return output_transcript_path

# extract transcript from audio
def extract_and_save_frames_and_metadata(
        path_to_video, 
        path_to_transcript, 
        path_to_save_extracted_frames,
        path_to_save_metadatas):
    
    # metadatas will store the metadata of all extracted frames
    metadatas = []

    # load video using cv2
    video = cv2.VideoCapture(path_to_video)
    # load transcript using webvtt
    trans = webvtt.read(path_to_transcript)
    
    # iterate transcript file
    # for each video segment specified in the transcript file
    for idx, transcript in enumerate(trans):
        # get the start time and end time in seconds
        start_time_ms = str2time(transcript.start)
        end_time_ms = str2time(transcript.end)
        # get the time in ms exactly 
        # in the middle of start time and end time
        mid_time_ms = (end_time_ms + start_time_ms) / 2
        # get the transcript, remove the next-line symbol
        text = transcript.text.replace("\n", ' ')
        # get frame at the middle time
        video.set(cv2.CAP_PROP_POS_MSEC, mid_time_ms)
        success, frame = video.read()
        if success:
            # if the frame is extracted successfully, resize it
            image = maintain_aspect_ratio_resize(frame, height=350)
            # save frame as JPEG file
            img_fname = f'frame_{idx}.jpg'
            img_fpath = osp.join(
                path_to_save_extracted_frames, img_fname
            )
            cv2.imwrite(img_fpath, image)

            # prepare the metadata
            metadata = {
                'extracted_frame_path': img_fpath,
                'transcript': text,
                'video_segment_id': idx,
                'video_path': path_to_video,
                'mid_time_ms': mid_time_ms,
            }
            metadatas.append(metadata)

        else:
            print(f"ERROR! Cannot extract frame: idx = {idx}")

    # save metadata of all extracted frames
    fn = osp.join(path_to_save_metadatas, 'metadatas.json')
    with open(fn, 'w', encoding='utf-8') as outfile:
        json.dump(metadatas, outfile, ensure_ascii=False)
    return metadatas

def refine_transcript_for_ingestion_and_inference_from_metadatas(
    metadatas,
    num_transcript_concat_for_ingesting: int = 2,
    num_transcript_concat_for_inference: int = 7,
):
    """
    Refine the transcript for ingestion and inference from metadata.
    Concatenates transcripts around each frame to create a larger context for embedding and inference models.
    """
    text_list = []
    image_list = []
    refined_metadatas = []
    for i, metadata in enumerate(metadatas):
        path_to_frame = metadata['extracted_frame_path']
        idx_frame = metadata['video_segment_id']
        lb_ingesting = max(0, idx_frame - num_transcript_concat_for_ingesting)
        ub_ingesting = min(len(metadatas), idx_frame + num_transcript_concat_for_ingesting + 1)
        caption_for_ingesting = " ".join(
            [metadatas[j]["transcript"] for j in range(lb_ingesting, ub_ingesting)]
        )

        lb_inference = max(0, i - num_transcript_concat_for_inference)
        ub_inference = min(len(metadatas), i + num_transcript_concat_for_inference + 1)
        caption_for_inference = " ".join(
            [metadatas[j]["transcript"] for j in range(lb_inference, ub_inference)]
        )

        time_of_frame = metadata["mid_time_ms"]
        embedding_type = "pair"
        text_list.append(caption_for_ingesting)
        image_list.append(path_to_frame)
        refined_metadatas.append(
            {
                "transcript": caption_for_ingesting,
                "extracted_frame_path": path_to_frame,
                "time_of_frame_ms": float(time_of_frame),
                # "embedding_type": embedding_type,
                "transcript_for_inference": caption_for_inference,
            }
        )
    return text_list, image_list, refined_metadatas

def ingest_text_image_pairs_to_vectorstore(
        texts: List[str],
        images: List[str | PIL.Image.Image],
        embedding: Embeddings,
        metadatas: Optional[List[dict]] = None,
        connection: Optional[Any] = None,
        table_name: Optional[str] = "vectorstore",
        mode: Optional[str] = "overwrite",
) -> MultimodalLanceDB:
    """ Ingest text-image pairs into a LanceDB vector store.
    Args:
        texts (List[str]): List of text descriptions.
        images (List[str | PIL.Image.Image]): List of image paths or PIL Image objects.
        embedding (Embeddings): Embedding model to use for generating vector representations.
        metadatas (Optional[List[dict]]): Optional list of metadata dictionaries for each text-image pair.
        connection (Optional[Any]): Optional LanceDB connection object.
        table_name (Optional[str]): Name of the table in the vector store.
        mode (Optional[str]): Mode for adding data to the vector store, e.g., "overwrite".
    Returns:
        MultimodalLanceDB: An instance of the MultimodalLanceDB class with the ingested data.
    """
    instance = MultimodalLanceDB.from_text_image_pairs(
        texts=texts,
        images=images,
        embedding=embedding,
        metadatas=metadatas,
        connection=connection,
        table_name=table_name,
        mode=mode,
    )
    return instance