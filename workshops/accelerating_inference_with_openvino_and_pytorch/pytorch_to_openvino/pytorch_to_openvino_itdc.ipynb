{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb1352db-ebeb-478a-9ece-3cc179d330a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Convert a PyTorch Model to OpenVINO™ IR\n",
    "\n",
    "This tutorial demonstrates step-by-step instructions on how to do inference on a PyTorch classification model using OpenVINO Runtime.\n",
    "Starting from OpenVINO 2023.0 release, OpenVINO supports direct PyTorch model conversion without an intermediate step to convert them into ONNX format. In order, if you try to use the lower OpenVINO version or prefer to use ONNX, please check this [tutorial](../pytorch-to-openvino/pytorch-onnx-to-openvino.ipynb).\n",
    "\n",
    "In this tutorial, we will use the [RegNetY_800MF](https://arxiv.org/abs/2003.13678) model from [torchvision](https://pytorch.org/vision/stable/index.html) to demonstrate how to convert PyTorch models to OpenVINO Intermediate Representation.\n",
    "\n",
    "The RegNet model was proposed in [Designing Network Design Spaces](https://arxiv.org/abs/2003.13678) by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollár. The authors design search spaces to perform Neural Architecture Search (NAS). They first start from a high dimensional search space and iteratively reduce the search space by empirically applying constraints based on the best-performing models sampled by the current search space.  Instead of focusing on designing individual network instances, authors design network design spaces that parametrize populations of networks. The overall process is analogous to the classic manual design of networks but elevated to the design space level. The RegNet design space provides simple and fast networks that work well across a wide range of flop regimes.\n",
    "\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Load PyTorch Model](#Load-PyTorch-Model)\n",
    "    - [Prepare Input Data](#Prepare-Input-Data)\n",
    "    - [Run PyTorch Model Inference](#Run-PyTorch-Model-Inference)\n",
    "    - [Benchmark PyTorch Model Inference](#Benchmark-PyTorch-Model-Inference)\n",
    "- [Convert PyTorch Model to OpenVINO Intermediate Representation](#Convert-PyTorch-Model-to-OpenVINO-Intermediate-Representation)\n",
    "    - [Select inference device](#Select-inference-device)\n",
    "    - [Run OpenVINO Model Inference](#Run-OpenVINO-Model-Inference)\n",
    "    - [Benchmark OpenVINO Model Inference](#Benchmark-OpenVINO-Model-Inference)\n",
    "- [Convert PyTorch Model with Static Input Shape](#Convert-PyTorch-Model-with-Static-Input-Shape)\n",
    "    - [Select inference device](#Select-inference-device)\n",
    "    - [Run OpenVINO Model Inference with Static Input Shape](#Run-OpenVINO-Model-Inference-with-Static-Input-Shape)\n",
    "    - [Benchmark OpenVINO Model Inference with Static Input Shape](#Benchmark-OpenVINO-Model-Inference-with-Static-Input-Shape)\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide).\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/pytorch-to-openvino/pytorch-to-openvino.ipynb\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f3f209-733d-4af7-978e-d970dfd25081",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c49bee-9296-47f7-82b9-a854a77112aa",
   "metadata": {},
   "source": [
    "Install notebook dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab2cfa4-65d6-44e5-93a5-16f5eeaa1d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install openvino>=2023.1.0 scipy Pillow torch torchvision --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "username = os.environ.get('USER')\n",
    "user_bin_path = os.path.expanduser(f\"/home/{username}/.local/bin\")\n",
    "sys.path.append(user_bin_path)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8497cf8",
   "metadata": {},
   "source": [
    "Please restart the kernel in order to make installed dependencies available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c7a8ae-b850-4f19-bc5b-0ee34f4484a3",
   "metadata": {},
   "source": [
    "Download input data and label map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f56f2346-5ab1-4f3b-93b8-eacf560d29f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "MODEL_DIR = Path(\"model\")\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "MODEL_NAME = \"regnet_y_800mf\"\n",
    "\n",
    "image = Image.open(requests.get(\"https://farm9.staticflickr.com/8225/8511402100_fea15da1c5_z.jpg\", stream=True).raw)\n",
    "\n",
    "labels_file = DATA_DIR / \"imagenet_2012.txt\"\n",
    "\n",
    "if not labels_file.exists():\n",
    "    resp = requests.get(\"https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/data/dataset_classes/imagenet_2012.txt\")\n",
    "    with labels_file.open(\"wb\") as f:\n",
    "        f.write(resp.content)\n",
    "\n",
    "imagenet_classes = labels_file.open(\"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7cf857-1983-4201-a778-4086754b4926",
   "metadata": {},
   "source": [
    "## Load PyTorch Model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Generally, PyTorch models represent an instance of the `torch.nn.Module` class, initialized by a state dictionary with model weights.\n",
    "Typical steps for getting a pre-trained model:\n",
    "\n",
    "1. Create an instance of a model class\n",
    "2. Load checkpoint state dict, which contains pre-trained model weights\n",
    "3. Turn the model to evaluation for switching some operations to inference mode\n",
    "\n",
    "The `torchvision` module provides a ready-to-use set of functions for model class initialization. We will use `torchvision.models.regnet_y_800mf`. You can directly pass pre-trained model weights to the model initialization function using the weights enum `RegNet_Y_800MF_Weights.DEFAULT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46ba1d36-77bb-40d4-a293-43587182681b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# get default weights using available weights Enum for model\n",
    "weights = torchvision.models.RegNet_Y_800MF_Weights.DEFAULT\n",
    "\n",
    "# create model topology and load weights\n",
    "model = torchvision.models.regnet_y_800mf(weights=weights)\n",
    "\n",
    "# switch model to inference mode\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3f4e43-3805-487a-8439-5928724d7e7b",
   "metadata": {},
   "source": [
    "### Prepare Input Data\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The code below demonstrates how to preprocess input data using a model-specific transforms module from `torchvision`. After transformation, we should concatenate images into batched tensor, in our case, we will run the model with batch 1, so we just unsqueeze input on the first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d26f6c3-49dc-48f3-a723-70499637a2c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize the Weight Transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Apply it to the input image\n",
    "img_transformed = preprocess(image)\n",
    "\n",
    "# Add batch dimension to image tensor\n",
    "input_tensor = img_transformed.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d936005-d805-4f90-bf3f-9eeef16f9551",
   "metadata": {},
   "source": [
    "### Run PyTorch Model Inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The model returns a vector of probabilities in raw logits format, softmax can be applied to get normalized values in the [0, 1] range. For a demonstration that the output of the original model and OpenVINO converted is the same, we defined a common postprocessing function which can be reused later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14394e67-949d-4828-a44b-4c64423bb61e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Perform model inference on input tensor\n",
    "result = model(input_tensor)\n",
    "\n",
    "\n",
    "# Postprocessing function for getting results in the same way for both PyTorch model inference and OpenVINO\n",
    "def postprocess_result(output_tensor: np.ndarray, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Posprocess model results. This function applied sofrmax on output tensor and returns specified top_k number of labels with highest probability\n",
    "    Parameters:\n",
    "      output_tensor (np.ndarray): model output tensor with probabilities\n",
    "      top_k (int, *optional*, default 5): number of labels with highest probability for return\n",
    "    Returns:\n",
    "      topk_labels: label ids for selected top_k scores\n",
    "      topk_scores: selected top_k highest scores predicted by model\n",
    "    \"\"\"\n",
    "    softmaxed_scores = softmax(output_tensor, -1)[0]\n",
    "    topk_labels = np.argsort(softmaxed_scores)[-top_k:][::-1]\n",
    "    topk_scores = softmaxed_scores[topk_labels]\n",
    "    return topk_labels, topk_scores\n",
    "\n",
    "\n",
    "# Postprocess results\n",
    "top_labels, top_scores = postprocess_result(result.detach().numpy())\n",
    "\n",
    "# Show results\n",
    "display(image)\n",
    "for idx, (label, score) in enumerate(zip(top_labels, top_scores)):\n",
    "    _, predicted_label = imagenet_classes[label].split(\" \", 1)\n",
    "    print(f\"{idx + 1}: {predicted_label} - {score * 100 :.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afecb7c-4418-410f-89a6-ead9f35866bd",
   "metadata": {},
   "source": [
    "### Benchmark PyTorch Model Inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd10d5a9-f5cb-4c33-8245-0ba8e928866d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# Run model inference\n",
    "model(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef75f58-644b-4f6e-8d20-4c200a67a0df",
   "metadata": {},
   "source": [
    "## Convert PyTorch Model to OpenVINO Intermediate Representation\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Starting from the 2023.0 release OpenVINO supports direct PyTorch models conversion to OpenVINO Intermediate Representation (IR) format. OpenVINO model conversion API should be used for these purposes. More details regarding PyTorch model conversion can be found in OpenVINO [documentation](https://docs.openvino.ai/2024/openvino-workflow/model-preparation/convert-model-pytorch.html)\n",
    "\n",
    "\n",
    "The `convert_model` function accepts the PyTorch model object and returns the `openvino.Model` instance ready to load on a device using `core.compile_model` or save on disk for next usage using `ov.save_model`. Optionally, we can provide additional parameters, such as:\n",
    "\n",
    "* `compress_to_fp16` - flag to perform model weights compression into FP16 data format. It may reduce the required space for model storage on disk and give speedup for inference devices, where FP16 calculation is supported.\n",
    "* `example_input` - input data sample which can be used for model tracing.\n",
    "* `input_shape` - the shape of input tensor for conversion\n",
    "\n",
    "and any other advanced options supported by model conversion Python API. More details can be found on this [page](https://docs.openvino.ai/2024/openvino-workflow/model-preparation/conversion-parameters.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d5d09b-2f87-4b19-acfd-5e5acc1a0197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "\n",
    "# Create OpenVINO Core object instance\n",
    "core = ov.Core()\n",
    "\n",
    "# Convert model to openvino.runtime.Model object\n",
    "ov_model = ov.convert_model(model)\n",
    "\n",
    "# Save openvino.runtime.Model object on disk\n",
    "ov.save_model(ov_model, MODEL_DIR / f\"{MODEL_NAME}_dynamic.xml\")\n",
    "\n",
    "ov_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41afb4f2-e385-45c0-a6d3-22d43da85e00",
   "metadata": {},
   "source": [
    "### Select inference device\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c126238d-58fa-40bf-8610-ae96d74e311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd104161-30ae-45b3-9c8e-6073afeb889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenVINO model on device\n",
    "compiled_model = core.compile_model(ov_model, device.value)\n",
    "compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56bdbb6-a2ad-49ce-8d79-4bf590c5c1ab",
   "metadata": {},
   "source": [
    "### Run OpenVINO Model Inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e4ed07-8044-48da-83ac-52bcb4d64e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model inference\n",
    "result = compiled_model(input_tensor)[0]\n",
    "\n",
    "# Posptorcess results\n",
    "top_labels, top_scores = postprocess_result(result)\n",
    "\n",
    "# Show results\n",
    "display(image)\n",
    "for idx, (label, score) in enumerate(zip(top_labels, top_scores)):\n",
    "    _, predicted_label = imagenet_classes[label].split(\" \", 1)\n",
    "    print(f\"{idx + 1}: {predicted_label} - {score * 100 :.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae13535e-7e7a-43c6-9144-1075b8dd02a2",
   "metadata": {},
   "source": [
    "### Benchmark OpenVINO Model Inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e73715-ff18-4e6f-a6ea-f1ae18178086",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "compiled_model(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a60b16-d12d-4b61-8be8-b25e39d30dbf",
   "metadata": {},
   "source": [
    "## Convert PyTorch Model with Static Input Shape\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa510788-ae07-4c0a-b919-7f7cef4d84a0",
   "metadata": {},
   "source": [
    "The default conversion path preserves dynamic input shapes, in order if you want to convert the model with static shapes, you can explicitly specify it during conversion using the `input_shape` parameter or reshape the model into the desired shape after conversion. For the model reshaping example please check the following [tutorial](../openvino-api/openvino-api.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe2ffc9-2e1a-42b5-96d2-8dcd51d8a8a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert model to openvino.runtime.Model object\n",
    "ov_model = ov.convert_model(model, input=[[1, 3, 224, 224]])\n",
    "# Save openvino.runtime.Model object on disk\n",
    "ov.save_model(ov_model, MODEL_DIR / f\"{MODEL_NAME}_static.xml\")\n",
    "ov_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a21db7-bb16-43ba-9e3b-7150252fa38e",
   "metadata": {},
   "source": [
    "### Select inference device\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c327696d-2186-4410-b0c2-84908583f1e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a40295d2b54577b9df4bd64d1c83d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=2, options=('CPU', 'GPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7564e17-3286-441f-9c8e-2907e86f4147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenVINO model on device\n",
    "compiled_model = core.compile_model(ov_model, device.value)\n",
    "compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8748a8f-f3fb-41e3-8aae-269f6aa5f56e",
   "metadata": {},
   "source": [
    "Now, we can see that input of our converted model is tensor of shape [1, 3, 224, 224] instead of [?, 3, ?, ?] reported by previously converted model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec01d431-6a12-4207-945e-1143d1805854",
   "metadata": {},
   "source": [
    "### Run OpenVINO Model Inference with Static Input Shape\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba94f1c5-34eb-4c8c-a295-32c5ef1e0e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model inference\n",
    "result = compiled_model(input_tensor)[0]\n",
    "\n",
    "# Posptorcess results\n",
    "top_labels, top_scores = postprocess_result(result)\n",
    "\n",
    "# Show results\n",
    "display(image)\n",
    "for idx, (label, score) in enumerate(zip(top_labels, top_scores)):\n",
    "    _, predicted_label = imagenet_classes[label].split(\" \", 1)\n",
    "    print(f\"{idx + 1}: {predicted_label} - {score * 100 :.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f91ccb-a928-48d2-9095-8a3d919bd2d9",
   "metadata": {},
   "source": [
    "### Benchmark OpenVINO Model Inference with Static Input Shape\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c3079c-dfa3-42ae-8df8-30eaf85b0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "compiled_model(input_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "openvino_notebooks": {
   "imageUrl": "https://user-images.githubusercontent.com/29454499/250586825-2a4a74a6-e091-4e47-8f29-59a72fe4975f.png",
   "tags": {
    "categories": [
     "Convert"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Image Classification"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
