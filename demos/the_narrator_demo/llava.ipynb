{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f17d903",
   "metadata": {},
   "source": [
    "# Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6281368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
      "Collecting openvino==2025.2.0 (from -r requirements.txt (line 3))\n",
      "  Using cached openvino-2025.2.0-19140-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting numpy==2.2.6 (from -r requirements.txt (line 4))\n",
      "  Using cached numpy-2.2.6-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting opencv-python==4.11.0.86 (from -r requirements.txt (line 5))\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl.metadata (20 kB)\n",
      "Collecting pillow==11.3.0 (from -r requirements.txt (line 6))\n",
      "  Using cached pillow-11.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting transformers==4.53.3 (from -r requirements.txt (line 7))\n",
      "  Using cached transformers-4.53.3-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting torch==2.8.0 (from -r requirements.txt (line 8))\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torch-2.8.0-cp312-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting torchvision==0.23.0 (from -r requirements.txt (line 9))\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torchvision-0.23.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting protobuf==5.28.2 (from -r requirements.txt (line 10))\n",
      "  Using cached protobuf-5.28.2-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting sentencepiece==0.2.0 (from -r requirements.txt (line 11))\n",
      "  Using cached sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting optimum-intel==1.25.1 (from -r requirements.txt (line 12))\n",
      "  Downloading optimum_intel-1.25.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting huggingface_hub==0.34.4 (from -r requirements.txt (line 13))\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting av==13.0.0 (from -r requirements.txt (line 14))\n",
      "  Using cached av-13.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.4 kB)\n",
      "Collecting openvino-telemetry>=2023.2.1 (from openvino==2025.2.0->-r requirements.txt (line 3))\n",
      "  Using cached openvino_telemetry-2025.2.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: packaging in /Users/emlanza/Repos/GitHub repos/openvino_build_deploy/.venv/lib/python3.12/site-packages (from openvino==2025.2.0->-r requirements.txt (line 3)) (25.0)\n",
      "Collecting filelock (from transformers==4.53.3->-r requirements.txt (line 7))\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers==4.53.3->-r requirements.txt (line 7))\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.53.3->-r requirements.txt (line 7))\n",
      "  Using cached regex-2025.7.34-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers==4.53.3->-r requirements.txt (line 7))\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.53.3->-r requirements.txt (line 7))\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers==4.53.3->-r requirements.txt (line 7))\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.53.3->-r requirements.txt (line 7))\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub==0.34.4->-r requirements.txt (line 13))\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface_hub==0.34.4->-r requirements.txt (line 13))\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub==0.34.4->-r requirements.txt (line 13))\n",
      "  Using cached hf_xet-1.1.7-cp37-abi3-macosx_11_0_arm64.whl.metadata (703 bytes)\n",
      "Collecting setuptools (from torch==2.8.0->-r requirements.txt (line 8))\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch==2.8.0->-r requirements.txt (line 8))\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.8.0->-r requirements.txt (line 8))\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch==2.8.0->-r requirements.txt (line 8))\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting optimum==1.27.* (from optimum-intel==1.25.1->-r requirements.txt (line 12))\n",
      "  Using cached optimum-1.27.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting datasets>=1.4.0 (from optimum-intel==1.25.1->-r requirements.txt (line 12))\n",
      "  Using cached datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting scipy (from optimum-intel==1.25.1->-r requirements.txt (line 12))\n",
      "  Using cached scipy-1.16.1-cp312-cp312-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting onnx (from optimum-intel==1.25.1->-r requirements.txt (line 12))\n",
      "  Using cached onnx-1.18.0-cp312-cp312-macosx_12_0_universal2.whl.metadata (6.9 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=1.4.0->optimum-intel==1.25.1->-r requirements.txt (line 12))\n",
      "  Using cached pyarrow-21.0.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=1.4.0->optimum-intel==1.25.1->-r requirements.txt (line 12))\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=1.4.0->optimum-intel==1.25.1->-r requirements.txt (line 12))\n",
      "  Using cached pandas-2.3.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting xxhash (from datasets>=1.4.0->optimum-intel==1.25.1->-r requirements.txt (line 12))\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=1.4.0->optimum-intel==1.25.1->-r requirements.txt (line 12))\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub==0.34.4->-r requirements.txt (line 13))\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.4.0->optimum-intel==1.25.1->-r requirements.txt (line 12))\n",
      "  Using cached aiohttp-3.12.15-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.4.0->optimum-intel==1.25.1->-r requirements.txt (line 12))\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.4.0->optimum-intel==1.25.1->-r requirements.txt (line 12))\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.4.0->optimum-intel==1.25.1->-r requirements.txt (line 12))\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.4.0->optimum-intel==1.25.1->-r requirements.txt (line 12))\n",
      "  Using cached frozenlist-1.7.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.4.0->optimum-intel==1.25.1->-r requirements.txt (line 12))\n",
      "  Downloading multidict-6.6.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.4.0->optimum-intel==1.25.1->-r requirements.txt (line 12))\n",
      "  Using cached propcache-0.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.4.0->optimum-intel==1.25.1->-r requirements.txt (line 12))\n",
      "  Using cached yarl-1.20.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (73 kB)\n",
      "Collecting idna>=2.0 (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.4.0->optimum-intel==1.25.1->-r requirements.txt (line 12))\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers==4.53.3->-r requirements.txt (line 7))\n",
      "  Downloading charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers==4.53.3->-r requirements.txt (line 7))\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers==4.53.3->-r requirements.txt (line 7))\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch==2.8.0->-r requirements.txt (line 8))\n",
      "  Using cached https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.8.0->-r requirements.txt (line 8))\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/emlanza/Repos/GitHub repos/openvino_build_deploy/.venv/lib/python3.12/site-packages (from pandas->datasets>=1.4.0->optimum-intel==1.25.1->-r requirements.txt (line 12)) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=1.4.0->optimum-intel==1.25.1->-r requirements.txt (line 12))\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=1.4.0->optimum-intel==1.25.1->-r requirements.txt (line 12))\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/emlanza/Repos/GitHub repos/openvino_build_deploy/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=1.4.0->optimum-intel==1.25.1->-r requirements.txt (line 12)) (1.17.0)\n",
      "Using cached openvino-2025.2.0-19140-cp312-cp312-macosx_11_0_arm64.whl (31.1 MB)\n",
      "Using cached numpy-2.2.6-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Using cached opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl (37.3 MB)\n",
      "Using cached pillow-11.3.0-cp312-cp312-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Downloading transformers-4.53.3-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torch-2.8.0-cp312-none-macosx_11_0_arm64.whl (73.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torchvision-0.23.0-cp312-cp312-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached protobuf-5.28.2-cp38-abi3-macosx_10_9_universal2.whl (414 kB)\n",
      "Using cached sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.2 MB)\n",
      "Downloading optimum_intel-1.25.1-py3-none-any.whl (359 kB)\n",
      "Downloading av-13.0.0-cp312-cp312-macosx_11_0_arm64.whl (19.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached hf_xet-1.1.7-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached optimum-1.27.0-py3-none-any.whl (425 kB)\n",
      "Using cached tokenizers-0.21.4-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached aiohttp-3.12.15-cp312-cp312-macosx_11_0_arm64.whl (469 kB)\n",
      "Downloading multidict-6.6.4-cp312-cp312-macosx_11_0_arm64.whl (43 kB)\n",
      "Using cached yarl-1.20.1-cp312-cp312-macosx_11_0_arm64.whl (89 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.7.0-cp312-cp312-macosx_11_0_arm64.whl (46 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached openvino_telemetry-2025.2.0-py3-none-any.whl (25 kB)\n",
      "Using cached propcache-0.3.2-cp312-cp312-macosx_11_0_arm64.whl (43 kB)\n",
      "Using cached pyarrow-21.0.0-cp312-cp312-macosx_12_0_arm64.whl (31.2 MB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached regex-2025.7.34-cp312-cp312-macosx_11_0_arm64.whl (286 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl (205 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached onnx-1.18.0-cp312-cp312-macosx_12_0_universal2.whl (18.3 MB)\n",
      "Using cached pandas-2.3.1-cp312-cp312-macosx_11_0_arm64.whl (10.7 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached scipy-1.16.1-cp312-cp312-macosx_14_0_arm64.whl (20.9 MB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: sentencepiece, pytz, openvino-telemetry, mpmath, xxhash, urllib3, tzdata, typing-extensions, tqdm, sympy, setuptools, safetensors, regex, pyyaml, pyarrow, protobuf, propcache, pillow, numpy, networkx, multidict, MarkupSafe, idna, hf-xet, fsspec, frozenlist, filelock, dill, charset_normalizer, certifi, av, attrs, aiohappyeyeballs, yarl, scipy, requests, pandas, openvino, opencv-python, onnx, multiprocess, jinja2, aiosignal, torch, huggingface_hub, aiohttp, torchvision, tokenizers, transformers, datasets, optimum, optimum-intel\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52/52\u001b[0m [optimum-intel]um-intel]um]]ers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 av-13.0.0 certifi-2025.8.3 charset_normalizer-3.4.3 datasets-4.0.0 dill-0.3.8 filelock-3.18.0 frozenlist-1.7.0 fsspec-2025.3.0 hf-xet-1.1.7 huggingface_hub-0.34.4 idna-3.10 jinja2-3.1.6 mpmath-1.3.0 multidict-6.6.4 multiprocess-0.70.16 networkx-3.5 numpy-2.2.6 onnx-1.18.0 opencv-python-4.11.0.86 openvino-2025.2.0 openvino-telemetry-2025.2.0 optimum-1.27.0 optimum-intel-1.25.1 pandas-2.3.1 pillow-11.3.0 propcache-0.3.2 protobuf-5.28.2 pyarrow-21.0.0 pytz-2025.2 pyyaml-6.0.2 regex-2025.7.34 requests-2.32.4 safetensors-0.6.2 scipy-1.16.1 sentencepiece-0.2.0 setuptools-80.9.0 sympy-1.14.0 tokenizers-0.21.4 torch-2.8.0 torchvision-0.23.0 tqdm-4.67.1 transformers-4.53.3 typing-extensions-4.14.1 tzdata-2025.2 urllib3-2.5.0 xxhash-3.5.0 yarl-1.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15651196",
   "metadata": {},
   "source": [
    "# Pre steps to Optimize and upload to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b1565c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emlanza/Repos/GitHub repos/openvino_build_deploy/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/emlanza/Repos/GitHub repos/openvino_build_deploy/.venv/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:162: OnnxExporterWarning: Symbolic function 'aten::scaled_dot_product_attention' already registered for opset 14. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 22.37it/s]\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Exporting a non-stateful decoder model currently results in a nan output in OpenVINO. There might be a performance impact due to the use of eager mask (floats) instead of sdpa mask (bools). \n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "/Users/emlanza/Repos/GitHub repos/openvino_build_deploy/.venv/lib/python3.12/site-packages/transformers/cache_utils.py:568: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  or not self.key_cache[layer_idx].numel()  # the layer has no cache\n",
      "/Users/emlanza/Repos/GitHub repos/openvino_build_deploy/.venv/lib/python3.12/site-packages/transformers/masking_utils.py:187: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (padding_length := kv_length + kv_offset - attention_mask.shape[-1]) > 0:\n",
      "/Users/emlanza/Repos/GitHub repos/openvino_build_deploy/.venv/lib/python3.12/site-packages/optimum/exporters/openvino/model_patcher.py:330: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask = torch.where(mask, torch.tensor(0.0, device=mask.device, dtype=dtype), torch.finfo(torch.float16).min)\n",
      "/Users/emlanza/Repos/GitHub repos/openvino_build_deploy/.venv/lib/python3.12/site-packages/transformers/cache_utils.py:552: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  not self.key_cache[layer_idx].numel()  # prefers not t.numel() to len(t) == 0 to export the model\n",
      "/Users/emlanza/Repos/GitHub repos/openvino_build_deploy/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:59: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, \"is_causal\", True)\n"
     ]
    }
   ],
   "source": [
    "# Save model to OpenVINO format\n",
    "\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "\n",
    "# First time: export and save\n",
    "model = OVModelForVisualCausalLM.from_pretrained(\n",
    "    \"llava-hf/LLaVA-NeXT-Video-7B-hf\", \n",
    "    export=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.save_pretrained(\"./llava_openvino_model\")\n",
    "\n",
    "# Future times: load from local saved version (much faster)\n",
    "model = OVModelForVisualCausalLM.from_pretrained(\"./llava_openvino_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a74a9c",
   "metadata": {},
   "source": [
    "## Upload OV model to HF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ede3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model card and upload to Hugging Face Hub\n",
    "\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "import os\n",
    "\n",
    "# Replace with your desired repo name\n",
    "REPO_NAME = \"llava-next-video-openvino\"  # Change this to your preferred name\n",
    "HF_USERNAME = \"ezelanza\"  # Replace with your HF username\n",
    "\n",
    "# Create model card content\n",
    "model_card = \"\"\"---\n",
    "license: apache-2.0\n",
    "base_model: llava-hf/LLaVA-NeXT-Video-7B-hf\n",
    "tags:\n",
    "- openvino\n",
    "- llava\n",
    "- multimodal\n",
    "- video\n",
    "- visual-question-answering\n",
    "---\n",
    "\n",
    "# LLaVA-NeXT-Video OpenVINO Model\n",
    "\n",
    "This is an OpenVINO optimized version of the LLaVA-NeXT-Video-7B-hf model.\n",
    "\n",
    "## Model Description\n",
    "- **Base Model**: llava-hf/LLaVA-NeXT-Video-7B-hf\n",
    "- **Optimization**: Converted to OpenVINO format for efficient inference\n",
    "- **Size**: ~7B parameters\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "\n",
    "model = OVModelForVisualCausalLM.from_pretrained(\"YOUR_USERNAME/llava-next-video-openvino\")\n",
    "```\n",
    "\n",
    "## License\n",
    "This model inherits the license from the original LLaVA-NeXT model.\n",
    "\"\"\"\n",
    "\n",
    "# Save model card\n",
    "with open(\"README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "print(\"Model card created: README.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d419ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload model to Hugging Face Hub\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "# Login to Hugging Face\n",
    "\n",
    "from huggingface_hub import login\n",
    "import getpass\n",
    "\n",
    "print(\"Go to: https://huggingface.co/settings/tokens\")\n",
    "print(\"Create a new token with WRITE permissions\")\n",
    "print()\n",
    "\n",
    "token = getpass.getpass(\"Enter your HF token: \")\n",
    "login(token=token)\n",
    "\n",
    "# Configuration - UPDATE THESE VALUES\n",
    "REPO_NAME = \"ezelanza/llava-next-video-openvino\"  # Your desired repo name\n",
    "# The username will be automatically detected from your login\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Create repository\n",
    "try:\n",
    "    repo_url = api.create_repo(\n",
    "        repo_id=REPO_NAME,\n",
    "        exist_ok=True,\n",
    "        repo_type=\"model\"\n",
    "    )\n",
    "    print(f\"Repository created/exists: {repo_url}\")\n",
    "except Exception as e:\n",
    "    print(f\"Repository creation error: {e}\")\n",
    "\n",
    "# Upload model files if they exist\n",
    "if os.path.exists(\"./llava_openvino_model\"):\n",
    "    print(\"Uploading model files...\")\n",
    "    api.upload_folder(\n",
    "        folder_path=\"./llava_openvino_model\",\n",
    "        repo_id=REPO_NAME,\n",
    "        repo_type=\"model\"\n",
    "    )\n",
    "    \n",
    "    # Upload README\n",
    "    if os.path.exists(\"README.md\"):\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=\"README.md\",\n",
    "            path_in_repo=\"README.md\",\n",
    "            repo_id=REPO_NAME,\n",
    "            repo_type=\"model\"\n",
    "        )\n",
    "    \n",
    "    print(f\"✅ Model uploaded successfully!\")\n",
    "    print(f\"🔗 View your model at: https://huggingface.co/{api.whoami()['name']}/{REPO_NAME}\")\n",
    "else:\n",
    "    print(\"❌ Model directory './llava_openvino_model' not found.\")\n",
    "    print(\"Run the first cell to save the model first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbf6f8f",
   "metadata": {},
   "source": [
    "## Optimize the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84aca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "from transformers import LlavaNextVideoProcessor\n",
    "from huggingface_hub import login\n",
    "import getpass\n",
    "\n",
    "print(\"Go to: https://huggingface.co/settings/tokens\")\n",
    "print(\"Create a new token with WRITE permissions\")\n",
    "print()\n",
    "\n",
    "token = getpass.getpass(\"Enter your HF token: \")\n",
    "login(token=token)\n",
    "model_id = \"ezelanza/llava-next-video-openvino\"\n",
    "\n",
    "\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id)\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a6d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel import OVQuantizationConfig, OVWeightQuantizationConfig, OVPipelineQuantizationConfig\n",
    "\n",
    "dataset, num_samples = \"contextual\", 50\n",
    "\n",
    "# weight-only 8bit\n",
    "woq_8bit = OVWeightQuantizationConfig(bits=8)\n",
    "\n",
    "# weight-only 4bit\n",
    "woq_4bit = OVWeightQuantizationConfig(bits=4, group_size=16)\n",
    "\n",
    "# static quantization\n",
    "static_8bit = OVQuantizationConfig(bits=8, dataset=dataset, num_samples=num_samples)\n",
    "\n",
    "# pipeline quantization: applying different quantization on each components\n",
    "ppl_q = OVPipelineQuantizationConfig(\n",
    "    quantization_configs={\n",
    "        \"lm_model\": OVQuantizationConfig(bits=8),\n",
    "        \"multimodal_model\": OVWeightQuantizationConfig(bits=8),\n",
    "        \"text_embeddings_model\": OVWeightQuantizationConfig(bits=8),\n",
    "        \"vision_embeddings_model\": OVWeightQuantizationConfig(bits=8),\n",
    "        \"vision_model\": OVWeightQuantizationConfig(bits=8) \n",
    "    },\n",
    "    dataset=dataset,\n",
    "    num_samples=num_samples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42edb4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel import OVModelForVisualCausalLM, OVWeightQuantizationConfig\n",
    "\n",
    "model_id = \"ezelanza/llava-next-video-openvino\"\n",
    "\n",
    "q_model = OVModelForVisualCausalLM.from_pretrained(model_id, quantization_config=woq_8bit)\n",
    "int8_model_path = \"llava_next_video_int8\"\n",
    "q_model.save_pretrained(int8_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload model to Hugging Face Hub\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "# Login to Hugging Face\n",
    "\n",
    "from huggingface_hub import login\n",
    "import getpass\n",
    "\n",
    "print(\"Go to: https://huggingface.co/settings/tokens\")\n",
    "print(\"Create a new token with WRITE permissions\")\n",
    "print()\n",
    "\n",
    "token = getpass.getpass(\"Enter your HF token: \")\n",
    "login(token=token)\n",
    "\n",
    "# Configuration - UPDATE THESE VALUES\n",
    "REPO_NAME = \"ezelanza/llava-next-video-openvino-int8\"  # Your desired repo name\n",
    "# The username will be automatically detected from your login\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Create repository\n",
    "try:\n",
    "    repo_url = api.create_repo(\n",
    "        repo_id=REPO_NAME,\n",
    "        exist_ok=True,\n",
    "        repo_type=\"model\"\n",
    "    )\n",
    "    print(f\"Repository created/exists: {repo_url}\")\n",
    "except Exception as e:\n",
    "    print(f\"Repository creation error: {e}\")\n",
    "\n",
    "# Upload model files if they exist\n",
    "if os.path.exists(\"./llava_next_video_int8\"):\n",
    "    print(\"Uploading model files...\")\n",
    "    api.upload_folder(\n",
    "        folder_path=\"./llava_next_video_int8\",\n",
    "        repo_id=REPO_NAME,\n",
    "        repo_type=\"model\"\n",
    "    )\n",
    "    \n",
    "    # Upload README\n",
    "    if os.path.exists(\"README.md\"):\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=\"README.md\",\n",
    "            path_in_repo=\"README.md\",\n",
    "            repo_id=REPO_NAME,\n",
    "            repo_type=\"model\"\n",
    "        )\n",
    "    \n",
    "    print(f\"✅ Model uploaded successfully!\")\n",
    "    print(f\"🔗 View your model at: https://huggingface.co/{api.whoami()['name']}/{REPO_NAME}\")\n",
    "else:\n",
    "    print(\"❌ Model directory './llava_openvino_model' not found.\")\n",
    "    print(\"Run the first cell to save the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0a662c",
   "metadata": {},
   "source": [
    "# Run inference with HF video dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed4fce2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eze/openvino/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to: https://huggingface.co/settings/tokens\n",
      "Create a new token with WRITE permissions\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download \n",
    "from transformers import LlavaNextVideoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "from transformers import LlavaNextVideoProcessor\n",
    "from huggingface_hub import login\n",
    "import getpass\n",
    "\n",
    "print(\"Go to: https://huggingface.co/settings/tokens\")\n",
    "print(\"Create a new token with WRITE permissions\")\n",
    "print()\n",
    "\n",
    "token = getpass.getpass(\"Enter your HF token: \")\n",
    "login(token=token)\n",
    "\n",
    "#load model in memory\n",
    "model_id = \"ezelanza/llava-next-video-openvino-int8\"\n",
    "\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8436ca96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: return_tensors.\n"
     ]
    }
   ],
   "source": [
    "video_path = hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\")\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What is happening in the video?\"},\n",
    "            {\"type\": \"video\", \"path\": video_path},\n",
    "            ],\n",
    "    },\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    conversation,\n",
    "    num_frames=4,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e27fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPTION GENERATED (video frames): In the video, we see a young child sitting on a bed, wearing glasses and engrossed in reading a book. The child appears to be focused on the book, possibly reading or looking at the pictures. The room has a cozy and lived-in feel, with various items\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**inputs, max_new_tokens=60)\n",
    "    \n",
    "response = processor.batch_decode(\n",
    "        output,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )[0]\n",
    "\n",
    "    \n",
    "if \"ASSISTANT:\" in response:\n",
    "        description = response.split(\"ASSISTANT:\")[-1].strip()\n",
    "else:\n",
    "        description = response.strip()\n",
    "    \n",
    "print(f\"CAPTION GENERATED (video frames): {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b5078",
   "metadata": {},
   "source": [
    "# Run inference with frames array (from HF video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db87875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eze/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "from transformers import LlavaNextVideoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "from huggingface_hub import hf_hub_download \n",
    "\n",
    "def extract_video_frames(video_path, num_frames=4, width=36, height=36):\n",
    "    \"\"\"Extract evenly spaced frames from a video file.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "        return []\n",
    "    \n",
    "    total_video_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    duration = total_video_frames / fps\n",
    "    \n",
    "    print(f\"Video info: {total_video_frames} frames, {fps:.1f} FPS, {duration:.1f} seconds\")\n",
    "    \n",
    "    # Calculate frame indices to extract (evenly spaced)\n",
    "    frame_indices = np.linspace(0, total_video_frames-1, num_frames, dtype=int)\n",
    "    \n",
    "    frames = []\n",
    "    for i, frame_idx in enumerate(frame_indices):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Resize frame to reduce processing time\n",
    "            frame_resized = cv2.resize(frame, (width, height))\n",
    "            # Convert BGR to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame_rgb)\n",
    "            print(f\"Extracted frame {i+1}/{num_frames} at frame {frame_idx}\")\n",
    "    \n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b50d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video info: 243 frames, 25.0 FPS, 9.7 seconds\n",
      "Extracted frame 1/4 at frame 0\n",
      "Extracted frame 2/4 at frame 80\n",
      "Extracted frame 3/4 at frame 161\n",
      "Extracted frame 4/4 at frame 242\n",
      "Saved frame 1 as video_frame_0.jpg\n",
      "Saved frame 2 as video_frame_1.jpg\n",
      "Saved frame 3 as video_frame_2.jpg\n",
      "Saved frame 4 as video_frame_3.jpg\n"
     ]
    }
   ],
   "source": [
    "video_path = hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\")\n",
    "\n",
    "# Extract frames from video\n",
    "frames = extract_video_frames(video_path, num_frames=4, width=120, height=80)\n",
    "    \n",
    "# Save frames as temporary images\n",
    "frame_paths = []\n",
    "for i, frame in enumerate(frames):\n",
    "    frame_path = f\"video_frame_{i}.jpg\"\n",
    "    cv2.imwrite(frame_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "    frame_paths.append(Path(frame_path))\n",
    "    print(f\"Saved frame {i+1} as {frame_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cf72dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Use frames as images in conversation\n",
    "#load model in memory\n",
    "model_id = \"ezelanza/llava-next-video-openvino-int8\"\n",
    "\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb24b07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPTION GENERATED (video frames): In the image, there is a young child sitting on a bed, engrossed in reading a book. The child is wearing glasses and appears to be focused on the content of the book, which is open in front of them. The child is dressed in a light-colored top\n"
     ]
    }
   ],
   "source": [
    "conversation_with_frames = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe what you see in these images. What is happening?\"},\n",
    "                *[{\"type\": \"image\", \"image\": path.as_posix()} for path in frame_paths],\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Process with the same model and processor\n",
    "inputs_with_frames = processor.apply_chat_template(\n",
    "        conversation_with_frames,\n",
    "        num_frames=1,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True\n",
    "    )\n",
    "    \n",
    "    # Generate response\n",
    "out_with_frames = model.generate(**inputs_with_frames, max_new_tokens=60)\n",
    "    \n",
    "response_with_frames = processor.batch_decode(\n",
    "        out_with_frames,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )[0]\n",
    "    \n",
    "if \"ASSISTANT:\" in response_with_frames:\n",
    "        description_with_frames = response_with_frames.split(\"ASSISTANT:\")[-1].strip()\n",
    "else:\n",
    "        description_with_frames = response_with_frames.strip()\n",
    "    \n",
    "print(f\"CAPTION GENERATED (video frames): {description_with_frames}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccebf8ae",
   "metadata": {},
   "source": [
    "# Run inference with frames array (from local webcam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c821f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eze/openvino/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to: https://huggingface.co/settings/tokens\n",
      "Create a new token with WRITE permissions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download \n",
    "from transformers import LlavaNextVideoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "from transformers import LlavaNextVideoProcessor\n",
    "from huggingface_hub import login\n",
    "import getpass\n",
    "\n",
    "print(\"Go to: https://huggingface.co/settings/tokens\")\n",
    "print(\"Create a new token with WRITE permissions\")\n",
    "print()\n",
    "\n",
    "token = getpass.getpass(\"Enter your HF token: \")\n",
    "login(token=token)\n",
    "\n",
    "#load model in memory\n",
    "model_id = \"ezelanza/llava-next-video-openvino-int8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fcb655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "from transformers import LlavaNextVideoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "\n",
    "\n",
    "def capture_webcam_to_mp4(output_path=\"output.mp4\", duration_seconds=3, fps=4, width=320, height=240):\n",
    "    \"\"\"Capture frames from webcam and save as an MP4 video.\"\"\"\n",
    "    cap = cv2.VideoCapture(0)  # 0 for default webcam\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam\")\n",
    "        return False\n",
    "    \n",
    "    # Set resolution to reduce processing time\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "    \n",
    "    # Verify the resolution was set\n",
    "    actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    print(f\"Webcam resolution set to: {actual_width}x{actual_height}\")\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Codec for .mp4\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (actual_width, actual_height))\n",
    "    \n",
    "    total_frames = int(duration_seconds * fps)\n",
    "    frame_interval = 1.4 / fps  # Time between frames\n",
    "    \n",
    "    print(f\"Capturing {total_frames} frames over {duration_seconds} seconds...\")\n",
    "    print(\"Starting in 3 seconds...\")\n",
    "    \n",
    "    # Countdown\n",
    "    for i in range(3, 0, -1):\n",
    "        print(f\"{i}...\")\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(\"Starting frame capture...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(total_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Warning: Frame {i} not captured correctly.\")\n",
    "            break\n",
    "        \n",
    "        # Resize frame to target size\n",
    "        frame_resized = cv2.resize(frame, (actual_width, actual_height))\n",
    "        \n",
    "        out.write(frame_resized)  # Write frame to video file\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Captured frame {i+1}/{total_frames} at {elapsed_time:.1f}s\")\n",
    "        \n",
    "        # Wait to maintain fps timing\n",
    "        if i < total_frames - 1:\n",
    "            time.sleep(frame_interval)\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Capture complete! Video saved to: {output_path}\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dca22f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam resolution set to: 320x240\n",
      "Capturing 12 frames over 3 seconds...\n",
      "Starting in 3 seconds...\n",
      "3...\n",
      "2...\n",
      "1...\n",
      "Starting frame capture...\n",
      "Captured frame 1/12 at 0.2s\n",
      "Captured frame 2/12 at 0.5s\n",
      "Captured frame 3/12 at 0.9s\n",
      "Captured frame 4/12 at 1.3s\n",
      "Captured frame 5/12 at 1.6s\n",
      "Captured frame 6/12 at 2.0s\n",
      "Captured frame 7/12 at 2.3s\n",
      "Captured frame 8/12 at 2.7s\n",
      "Captured frame 9/12 at 3.0s\n",
      "Captured frame 10/12 at 3.4s\n",
      "Captured frame 11/12 at 3.7s\n",
      "Captured frame 12/12 at 4.1s\n",
      "Capture complete! Video saved to: inference.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id)\n",
    "\n",
    "output_path=\"inference.mp4\"\n",
    "capture_webcam_to_mp4(output_path,duration_seconds=3,fps=4)\n",
    "    \n",
    "conversation_webcam = [\n",
    "    {\n",
    "\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What is happening in the video?\"},\n",
    "            {\"type\": \"video\", \"path\": output_path},\n",
    "            ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22ad51e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: return_tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPTION GENERATED: In the video, we see a man wearing glasses and a black jacket who appears to be in a room with a plant in the background. He is holding up his hand and making a gesture that could be interpreted as a wave or a greeting. The man seems to be in a\n"
     ]
    }
   ],
   "source": [
    "inputs_webcam = processor.apply_chat_template(\n",
    "    conversation_webcam,\n",
    "    num_frames=4,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "out_webcam = model.generate(**inputs_webcam, max_new_tokens=60)\n",
    "\n",
    "response_webcam = processor.batch_decode(\n",
    "                    out_webcam,\n",
    "                    skip_special_tokens=True,\n",
    "                    clean_up_tokenization_spaces=True\n",
    "                )[0]\n",
    "\n",
    "if \"ASSISTANT:\" in response_webcam:\n",
    "    description_webcam = response_webcam.split(\"ASSISTANT:\")[-1].strip()\n",
    "else:\n",
    "    # If no ASSISTANT marker, use the full response\n",
    "    description_webcam = response_webcam.strip()\n",
    "\n",
    "print(f\"CAPTION GENERATED: {description_webcam}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
