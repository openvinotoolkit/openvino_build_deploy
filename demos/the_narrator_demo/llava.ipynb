{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f17d903",
   "metadata": {},
   "source": [
    "# Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6281368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openvino==2025.2.0 in /home/eze/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (2025.2.0)\n",
      "Requirement already satisfied: numpy==2.2.6 in /home/eze/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.2.6)\n",
      "Requirement already satisfied: opencv-python==4.11.0.86 in /home/eze/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (4.11.0.86)\n",
      "Requirement already satisfied: pillow==11.3.0 in /home/eze/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (11.3.0)\n",
      "Requirement already satisfied: transformers==4.52.4 in /home/eze/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (4.52.4)\n",
      "Requirement already satisfied: torch==2.7.1 in /home/eze/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (2.7.1)\n",
      "Requirement already satisfied: torchvision==0.22.1 in /home/eze/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (0.22.1)\n",
      "Requirement already satisfied: protobuf==5.28.2 in /home/eze/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (5.28.2)\n",
      "Requirement already satisfied: sentencepiece==0.2.0 in /home/eze/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: optimum-intel==1.24.0 in /home/eze/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (1.24.0)\n",
      "Requirement already satisfied: huggingface_hub in /home/eze/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (0.33.5)\n",
      "Requirement already satisfied: nncf in /home/eze/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (2.17.0)\n",
      "Requirement already satisfied: av in /home/eze/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (15.0.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement hf_hub_download (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for hf_hub_download\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15651196",
   "metadata": {},
   "source": [
    "# Pre steps to Optimize and upload to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b1565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to OpenVINO format\n",
    "\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "\n",
    "# First time: export and save\n",
    "model = OVModelForVisualCausalLM.from_pretrained(\n",
    "    \"llava-hf/LLaVA-NeXT-Video-7B-hf\", \n",
    "    export=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.save_pretrained(\"./llava_openvino_model\")\n",
    "\n",
    "# Future times: load from local saved version (much faster)\n",
    "model = OVModelForVisualCausalLM.from_pretrained(\"./llava_openvino_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a74a9c",
   "metadata": {},
   "source": [
    "## Upload OV model to HF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ede3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model card and upload to Hugging Face Hub\n",
    "\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "import os\n",
    "\n",
    "# Replace with your desired repo name\n",
    "REPO_NAME = \"llava-next-video-openvino\"  # Change this to your preferred name\n",
    "HF_USERNAME = \"ezelanza\"  # Replace with your HF username\n",
    "\n",
    "# Create model card content\n",
    "model_card = \"\"\"---\n",
    "license: apache-2.0\n",
    "base_model: llava-hf/LLaVA-NeXT-Video-7B-hf\n",
    "tags:\n",
    "- openvino\n",
    "- llava\n",
    "- multimodal\n",
    "- video\n",
    "- visual-question-answering\n",
    "---\n",
    "\n",
    "# LLaVA-NeXT-Video OpenVINO Model\n",
    "\n",
    "This is an OpenVINO optimized version of the LLaVA-NeXT-Video-7B-hf model.\n",
    "\n",
    "## Model Description\n",
    "- **Base Model**: llava-hf/LLaVA-NeXT-Video-7B-hf\n",
    "- **Optimization**: Converted to OpenVINO format for efficient inference\n",
    "- **Size**: ~7B parameters\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "\n",
    "model = OVModelForVisualCausalLM.from_pretrained(\"YOUR_USERNAME/llava-next-video-openvino\")\n",
    "```\n",
    "\n",
    "## License\n",
    "This model inherits the license from the original LLaVA-NeXT model.\n",
    "\"\"\"\n",
    "\n",
    "# Save model card\n",
    "with open(\"README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "print(\"Model card created: README.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d419ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload model to Hugging Face Hub\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "# Login to Hugging Face\n",
    "\n",
    "from huggingface_hub import login\n",
    "import getpass\n",
    "\n",
    "print(\"Go to: https://huggingface.co/settings/tokens\")\n",
    "print(\"Create a new token with WRITE permissions\")\n",
    "print()\n",
    "\n",
    "token = getpass.getpass(\"Enter your HF token: \")\n",
    "login(token=token)\n",
    "\n",
    "# Configuration - UPDATE THESE VALUES\n",
    "REPO_NAME = \"ezelanza/llava-next-video-openvino\"  # Your desired repo name\n",
    "# The username will be automatically detected from your login\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Create repository\n",
    "try:\n",
    "    repo_url = api.create_repo(\n",
    "        repo_id=REPO_NAME,\n",
    "        exist_ok=True,\n",
    "        repo_type=\"model\"\n",
    "    )\n",
    "    print(f\"Repository created/exists: {repo_url}\")\n",
    "except Exception as e:\n",
    "    print(f\"Repository creation error: {e}\")\n",
    "\n",
    "# Upload model files if they exist\n",
    "if os.path.exists(\"./llava_openvino_model\"):\n",
    "    print(\"Uploading model files...\")\n",
    "    api.upload_folder(\n",
    "        folder_path=\"./llava_openvino_model\",\n",
    "        repo_id=REPO_NAME,\n",
    "        repo_type=\"model\"\n",
    "    )\n",
    "    \n",
    "    # Upload README\n",
    "    if os.path.exists(\"README.md\"):\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=\"README.md\",\n",
    "            path_in_repo=\"README.md\",\n",
    "            repo_id=REPO_NAME,\n",
    "            repo_type=\"model\"\n",
    "        )\n",
    "    \n",
    "    print(f\"‚úÖ Model uploaded successfully!\")\n",
    "    print(f\"üîó View your model at: https://huggingface.co/{api.whoami()['name']}/{REPO_NAME}\")\n",
    "else:\n",
    "    print(\"‚ùå Model directory './llava_openvino_model' not found.\")\n",
    "    print(\"Run the first cell to save the model first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbf6f8f",
   "metadata": {},
   "source": [
    "## Optimize the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84aca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "from transformers import LlavaNextVideoProcessor\n",
    "from huggingface_hub import login\n",
    "import getpass\n",
    "\n",
    "print(\"Go to: https://huggingface.co/settings/tokens\")\n",
    "print(\"Create a new token with WRITE permissions\")\n",
    "print()\n",
    "\n",
    "token = getpass.getpass(\"Enter your HF token: \")\n",
    "login(token=token)\n",
    "model_id = \"ezelanza/llava-next-video-openvino\"\n",
    "\n",
    "\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id)\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a6d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel import OVQuantizationConfig, OVWeightQuantizationConfig, OVPipelineQuantizationConfig\n",
    "\n",
    "dataset, num_samples = \"contextual\", 50\n",
    "\n",
    "# weight-only 8bit\n",
    "woq_8bit = OVWeightQuantizationConfig(bits=8)\n",
    "\n",
    "# weight-only 4bit\n",
    "woq_4bit = OVWeightQuantizationConfig(bits=4, group_size=16)\n",
    "\n",
    "# static quantization\n",
    "static_8bit = OVQuantizationConfig(bits=8, dataset=dataset, num_samples=num_samples)\n",
    "\n",
    "# pipeline quantization: applying different quantization on each components\n",
    "ppl_q = OVPipelineQuantizationConfig(\n",
    "    quantization_configs={\n",
    "        \"lm_model\": OVQuantizationConfig(bits=8),\n",
    "        \"multimodal_model\": OVWeightQuantizationConfig(bits=8),\n",
    "        \"text_embeddings_model\": OVWeightQuantizationConfig(bits=8),\n",
    "        \"vision_embeddings_model\": OVWeightQuantizationConfig(bits=8),\n",
    "        \"vision_model\": OVWeightQuantizationConfig(bits=8) \n",
    "    },\n",
    "    dataset=dataset,\n",
    "    num_samples=num_samples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42edb4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel import OVModelForVisualCausalLM, OVWeightQuantizationConfig\n",
    "\n",
    "model_id = \"ezelanza/llava-next-video-openvino\"\n",
    "\n",
    "q_model = OVModelForVisualCausalLM.from_pretrained(model_id, quantization_config=woq_8bit)\n",
    "int8_model_path = \"llava_next_video_int8\"\n",
    "q_model.save_pretrained(int8_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload model to Hugging Face Hub\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "# Login to Hugging Face\n",
    "\n",
    "from huggingface_hub import login\n",
    "import getpass\n",
    "\n",
    "print(\"Go to: https://huggingface.co/settings/tokens\")\n",
    "print(\"Create a new token with WRITE permissions\")\n",
    "print()\n",
    "\n",
    "token = getpass.getpass(\"Enter your HF token: \")\n",
    "login(token=token)\n",
    "\n",
    "# Configuration - UPDATE THESE VALUES\n",
    "REPO_NAME = \"ezelanza/llava-next-video-openvino-int8\"  # Your desired repo name\n",
    "# The username will be automatically detected from your login\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Create repository\n",
    "try:\n",
    "    repo_url = api.create_repo(\n",
    "        repo_id=REPO_NAME,\n",
    "        exist_ok=True,\n",
    "        repo_type=\"model\"\n",
    "    )\n",
    "    print(f\"Repository created/exists: {repo_url}\")\n",
    "except Exception as e:\n",
    "    print(f\"Repository creation error: {e}\")\n",
    "\n",
    "# Upload model files if they exist\n",
    "if os.path.exists(\"./llava_next_video_int8\"):\n",
    "    print(\"Uploading model files...\")\n",
    "    api.upload_folder(\n",
    "        folder_path=\"./llava_next_video_int8\",\n",
    "        repo_id=REPO_NAME,\n",
    "        repo_type=\"model\"\n",
    "    )\n",
    "    \n",
    "    # Upload README\n",
    "    if os.path.exists(\"README.md\"):\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=\"README.md\",\n",
    "            path_in_repo=\"README.md\",\n",
    "            repo_id=REPO_NAME,\n",
    "            repo_type=\"model\"\n",
    "        )\n",
    "    \n",
    "    print(f\"‚úÖ Model uploaded successfully!\")\n",
    "    print(f\"üîó View your model at: https://huggingface.co/{api.whoami()['name']}/{REPO_NAME}\")\n",
    "else:\n",
    "    print(\"‚ùå Model directory './llava_openvino_model' not found.\")\n",
    "    print(\"Run the first cell to save the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0a662c",
   "metadata": {},
   "source": [
    "# Run inference with HF video dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed4fce2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eze/openvino/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to: https://huggingface.co/settings/tokens\n",
      "Create a new token with WRITE permissions\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download \n",
    "from transformers import LlavaNextVideoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "from transformers import LlavaNextVideoProcessor\n",
    "from huggingface_hub import login\n",
    "import getpass\n",
    "\n",
    "print(\"Go to: https://huggingface.co/settings/tokens\")\n",
    "print(\"Create a new token with WRITE permissions\")\n",
    "print()\n",
    "\n",
    "token = getpass.getpass(\"Enter your HF token: \")\n",
    "login(token=token)\n",
    "\n",
    "#load model in memory\n",
    "model_id = \"ezelanza/llava-next-video-openvino-int8\"\n",
    "\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8436ca96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: return_tensors.\n"
     ]
    }
   ],
   "source": [
    "video_path = hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\")\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What is happening in the video?\"},\n",
    "            {\"type\": \"video\", \"path\": video_path},\n",
    "            ],\n",
    "    },\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    conversation,\n",
    "    num_frames=4,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e27fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPTION GENERATED (video frames): In the video, we see a young child sitting on a bed, wearing glasses and engrossed in reading a book. The child appears to be focused on the book, possibly reading or looking at the pictures. The room has a cozy and lived-in feel, with various items\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**inputs, max_new_tokens=60)\n",
    "    \n",
    "response = processor.batch_decode(\n",
    "        output,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )[0]\n",
    "\n",
    "    \n",
    "if \"ASSISTANT:\" in response:\n",
    "        description = response.split(\"ASSISTANT:\")[-1].strip()\n",
    "else:\n",
    "        description = response.strip()\n",
    "    \n",
    "print(f\"CAPTION GENERATED (video frames): {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b5078",
   "metadata": {},
   "source": [
    "# Run inference with frames array (from HF video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db87875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eze/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "from transformers import LlavaNextVideoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "from huggingface_hub import hf_hub_download \n",
    "\n",
    "def extract_video_frames(video_path, num_frames=4, width=36, height=36):\n",
    "    \"\"\"Extract evenly spaced frames from a video file.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "        return []\n",
    "    \n",
    "    total_video_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    duration = total_video_frames / fps\n",
    "    \n",
    "    print(f\"Video info: {total_video_frames} frames, {fps:.1f} FPS, {duration:.1f} seconds\")\n",
    "    \n",
    "    # Calculate frame indices to extract (evenly spaced)\n",
    "    frame_indices = np.linspace(0, total_video_frames-1, num_frames, dtype=int)\n",
    "    \n",
    "    frames = []\n",
    "    for i, frame_idx in enumerate(frame_indices):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Resize frame to reduce processing time\n",
    "            frame_resized = cv2.resize(frame, (width, height))\n",
    "            # Convert BGR to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame_rgb)\n",
    "            print(f\"Extracted frame {i+1}/{num_frames} at frame {frame_idx}\")\n",
    "    \n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b50d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video info: 243 frames, 25.0 FPS, 9.7 seconds\n",
      "Extracted frame 1/4 at frame 0\n",
      "Extracted frame 2/4 at frame 80\n",
      "Extracted frame 3/4 at frame 161\n",
      "Extracted frame 4/4 at frame 242\n",
      "Saved frame 1 as video_frame_0.jpg\n",
      "Saved frame 2 as video_frame_1.jpg\n",
      "Saved frame 3 as video_frame_2.jpg\n",
      "Saved frame 4 as video_frame_3.jpg\n"
     ]
    }
   ],
   "source": [
    "video_path = hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\")\n",
    "\n",
    "# Extract frames from video\n",
    "frames = extract_video_frames(video_path, num_frames=4, width=120, height=80)\n",
    "    \n",
    "# Save frames as temporary images\n",
    "frame_paths = []\n",
    "for i, frame in enumerate(frames):\n",
    "    frame_path = f\"video_frame_{i}.jpg\"\n",
    "    cv2.imwrite(frame_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "    frame_paths.append(Path(frame_path))\n",
    "    print(f\"Saved frame {i+1} as {frame_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cf72dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Use frames as images in conversation\n",
    "#load model in memory\n",
    "model_id = \"ezelanza/llava-next-video-openvino-int8\"\n",
    "\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb24b07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPTION GENERATED (video frames): In the image, there is a young child sitting on a bed, engrossed in reading a book. The child is wearing glasses and appears to be focused on the content of the book, which is open in front of them. The child is dressed in a light-colored top\n"
     ]
    }
   ],
   "source": [
    "conversation_with_frames = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe what you see in these images. What is happening?\"},\n",
    "                *[{\"type\": \"image\", \"image\": path.as_posix()} for path in frame_paths],\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Process with the same model and processor\n",
    "inputs_with_frames = processor.apply_chat_template(\n",
    "        conversation_with_frames,\n",
    "        num_frames=1,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True\n",
    "    )\n",
    "    \n",
    "    # Generate response\n",
    "out_with_frames = model.generate(**inputs_with_frames, max_new_tokens=60)\n",
    "    \n",
    "response_with_frames = processor.batch_decode(\n",
    "        out_with_frames,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )[0]\n",
    "    \n",
    "if \"ASSISTANT:\" in response_with_frames:\n",
    "        description_with_frames = response_with_frames.split(\"ASSISTANT:\")[-1].strip()\n",
    "else:\n",
    "        description_with_frames = response_with_frames.strip()\n",
    "    \n",
    "print(f\"CAPTION GENERATED (video frames): {description_with_frames}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccebf8ae",
   "metadata": {},
   "source": [
    "# Run inference with frames array (from local webcam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c821f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eze/openvino/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to: https://huggingface.co/settings/tokens\n",
      "Create a new token with WRITE permissions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download \n",
    "from transformers import LlavaNextVideoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "from transformers import LlavaNextVideoProcessor\n",
    "from huggingface_hub import login\n",
    "import getpass\n",
    "\n",
    "print(\"Go to: https://huggingface.co/settings/tokens\")\n",
    "print(\"Create a new token with WRITE permissions\")\n",
    "print()\n",
    "\n",
    "token = getpass.getpass(\"Enter your HF token: \")\n",
    "login(token=token)\n",
    "\n",
    "#load model in memory\n",
    "model_id = \"ezelanza/llava-next-video-openvino-int8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fcb655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "from transformers import LlavaNextVideoProcessor\n",
    "from optimum.intel.openvino import OVModelForVisualCausalLM\n",
    "\n",
    "\n",
    "def capture_webcam_to_mp4(output_path=\"output.mp4\", duration_seconds=3, fps=4, width=320, height=240):\n",
    "    \"\"\"Capture frames from webcam and save as an MP4 video.\"\"\"\n",
    "    cap = cv2.VideoCapture(0)  # 0 for default webcam\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam\")\n",
    "        return False\n",
    "    \n",
    "    # Set resolution to reduce processing time\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "    \n",
    "    # Verify the resolution was set\n",
    "    actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    print(f\"Webcam resolution set to: {actual_width}x{actual_height}\")\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Codec for .mp4\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (actual_width, actual_height))\n",
    "    \n",
    "    total_frames = int(duration_seconds * fps)\n",
    "    frame_interval = 1.4 / fps  # Time between frames\n",
    "    \n",
    "    print(f\"Capturing {total_frames} frames over {duration_seconds} seconds...\")\n",
    "    print(\"Starting in 3 seconds...\")\n",
    "    \n",
    "    # Countdown\n",
    "    for i in range(3, 0, -1):\n",
    "        print(f\"{i}...\")\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(\"Starting frame capture...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(total_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Warning: Frame {i} not captured correctly.\")\n",
    "            break\n",
    "        \n",
    "        # Resize frame to target size\n",
    "        frame_resized = cv2.resize(frame, (actual_width, actual_height))\n",
    "        \n",
    "        out.write(frame_resized)  # Write frame to video file\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Captured frame {i+1}/{total_frames} at {elapsed_time:.1f}s\")\n",
    "        \n",
    "        # Wait to maintain fps timing\n",
    "        if i < total_frames - 1:\n",
    "            time.sleep(frame_interval)\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Capture complete! Video saved to: {output_path}\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dca22f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam resolution set to: 320x240\n",
      "Capturing 12 frames over 3 seconds...\n",
      "Starting in 3 seconds...\n",
      "3...\n",
      "2...\n",
      "1...\n",
      "Starting frame capture...\n",
      "Captured frame 1/12 at 0.2s\n",
      "Captured frame 2/12 at 0.5s\n",
      "Captured frame 3/12 at 0.9s\n",
      "Captured frame 4/12 at 1.3s\n",
      "Captured frame 5/12 at 1.6s\n",
      "Captured frame 6/12 at 2.0s\n",
      "Captured frame 7/12 at 2.3s\n",
      "Captured frame 8/12 at 2.7s\n",
      "Captured frame 9/12 at 3.0s\n",
      "Captured frame 10/12 at 3.4s\n",
      "Captured frame 11/12 at 3.7s\n",
      "Captured frame 12/12 at 4.1s\n",
      "Capture complete! Video saved to: inference.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n",
    "model = OVModelForVisualCausalLM.from_pretrained(model_id)\n",
    "\n",
    "output_path=\"inference.mp4\"\n",
    "capture_webcam_to_mp4(output_path,duration_seconds=3,fps=4)\n",
    "    \n",
    "conversation_webcam = [\n",
    "    {\n",
    "\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What is happening in the video?\"},\n",
    "            {\"type\": \"video\", \"path\": output_path},\n",
    "            ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22ad51e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: return_tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPTION GENERATED: In the video, we see a man wearing glasses and a black jacket who appears to be in a room with a plant in the background. He is holding up his hand and making a gesture that could be interpreted as a wave or a greeting. The man seems to be in a\n"
     ]
    }
   ],
   "source": [
    "inputs_webcam = processor.apply_chat_template(\n",
    "    conversation_webcam,\n",
    "    num_frames=4,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "out_webcam = model.generate(**inputs_webcam, max_new_tokens=60)\n",
    "\n",
    "response_webcam = processor.batch_decode(\n",
    "                    out_webcam,\n",
    "                    skip_special_tokens=True,\n",
    "                    clean_up_tokenization_spaces=True\n",
    "                )[0]\n",
    "\n",
    "if \"ASSISTANT:\" in response_webcam:\n",
    "    description_webcam = response_webcam.split(\"ASSISTANT:\")[-1].strip()\n",
    "else:\n",
    "    # If no ASSISTANT marker, use the full response\n",
    "    description_webcam = response_webcam.strip()\n",
    "\n",
    "print(f\"CAPTION GENERATED: {description_webcam}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
